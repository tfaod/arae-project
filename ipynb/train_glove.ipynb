{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, Corpus, batchify\n",
    "from model_glove import Seq2Seq2Decoder, Seq2Seq, MLP_G, MLP_Classify\n",
    "from models import MLP_D\n",
    "import shutil\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': 'data_yelp', 'outf': '_output', 'load_vocab': '', 'vocab_size': 30000, 'maxlen': 25, 'lowercase': True, 'emsize': 300, 'nhidden': 300, 'nlayers': 1, 'noise_r': 0.1, 'noise_anneal': 0.9995, 'hidden_init': False, 'arch_g': '300-300', 'arch_d': '300-300', 'arch_classify': '300-300', 'z_size': 32, 'temp': 1, 'dropout': 0.0, 'epochs': 25, 'batch_size': 64, 'niters_ae': 1, 'niters_gan_d': 5, 'niters_gan_g': 1, 'niters_gan_ae': 1, 'niters_gan_schedule': '', 'lr_ae': 1, 'lr_gan_g': 0.0001, 'lr_gan_d': 0.0001, 'lr_classify': 0.0001, 'beta1': 0.5, 'clip': 1, 'gan_gp_lambda': 0.1, 'grad_lambda': 0.01, 'lambda_class': 1, 'sample': False, 'log_interval': 1, 'seed': 1111, 'cuda': True, 'device_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='ARAE for Yelp transfer')\n",
    "# Path Arguments\n",
    "parser.add_argument('--data_path', type=str, required=True,\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--outf', type=str, default=\"_output\",\n",
    "                    help='output directory name')\n",
    "parser.add_argument('--load_vocab', type=str, default=\"\",\n",
    "                    help='path to load vocabulary from')\n",
    "\n",
    "# Data Processing Arguments\n",
    "parser.add_argument('--vocab_size', type=int, default=30000,\n",
    "                    help='cut vocabulary down to this size '\n",
    "                         '(most frequently seen words in train)')\n",
    "parser.add_argument('--maxlen', type=int, default=25,\n",
    "                    help='maximum sentence length')\n",
    "parser.add_argument('--lowercase', dest='lowercase', action='store_true',\n",
    "                    help='lowercase all text')\n",
    "parser.add_argument('--no-lowercase', dest='lowercase', action='store_true',\n",
    "                    help='not lowercase all text')\n",
    "parser.set_defaults(lowercase=True)\n",
    "\n",
    "# Model Arguments\n",
    "parser.add_argument('--emsize', type=int, default=300,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhidden', type=int, default=300,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=1,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--noise_r', type=float, default=0.1,\n",
    "                    help='stdev of noise for autoencoder (regularizer)')\n",
    "parser.add_argument('--noise_anneal', type=float, default=0.9995,\n",
    "                    help='anneal noise_r exponentially by this'\n",
    "                         'every 100 iterations')\n",
    "parser.add_argument('--hidden_init', action='store_true',\n",
    "                    help=\"initialize decoder hidden state with encoder's\")\n",
    "parser.add_argument('--arch_g', type=str, default='300-300',\n",
    "                    help='generator architecture (MLP)')\n",
    "parser.add_argument('--arch_d', type=str, default='300-300',\n",
    "                    help='critic/discriminator architecture (MLP)')\n",
    "parser.add_argument('--arch_classify', type=str, default='300-300',\n",
    "                    help='classifier architecture')\n",
    "parser.add_argument('--z_size', type=int, default=32,\n",
    "                    help='dimension of random noise z to feed into generator')\n",
    "parser.add_argument('--temp', type=float, default=1,\n",
    "                    help='softmax temperature (lower --> more discrete)')\n",
    "parser.add_argument('--dropout', type=float, default=0.0,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "\n",
    "# Training Arguments\n",
    "parser.add_argument('--epochs', type=int, default=25,\n",
    "                    help='maximum number of epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--niters_ae', type=int, default=1,\n",
    "                    help='number of autoencoder iterations in training')\n",
    "parser.add_argument('--niters_gan_d', type=int, default=5,\n",
    "                    help='number of discriminator iterations in training')\n",
    "parser.add_argument('--niters_gan_g', type=int, default=1,\n",
    "                    help='number of generator iterations in training')\n",
    "parser.add_argument('--niters_gan_ae', type=int, default=1,\n",
    "                    help='number of gan-into-ae iterations in training')\n",
    "parser.add_argument('--niters_gan_schedule', type=str, default='',\n",
    "                    help='epoch counts to increase number of GAN training '\n",
    "                         ' iterations (increment by 1 each time)')\n",
    "parser.add_argument('--lr_ae', type=float, default=1,\n",
    "                    help='autoencoder learning rate')\n",
    "parser.add_argument('--lr_gan_g', type=float, default=1e-04,\n",
    "                    help='generator learning rate')\n",
    "parser.add_argument('--lr_gan_d', type=float, default=1e-04,\n",
    "                    help='critic/discriminator learning rate')\n",
    "parser.add_argument('--lr_classify', type=float, default=1e-04,\n",
    "                    help='classifier learning rate')\n",
    "parser.add_argument('--beta1', type=float, default=0.5,\n",
    "                    help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--clip', type=float, default=1,\n",
    "                    help='gradient clipping, max norm')\n",
    "parser.add_argument('--gan_gp_lambda', type=float, default=0.1,\n",
    "                    help='WGAN GP penalty lambda')\n",
    "parser.add_argument('--grad_lambda', type=float, default=0.01,\n",
    "                    help='WGAN into AE lambda')\n",
    "parser.add_argument('--lambda_class', type=float, default=1,\n",
    "                    help='lambda on classifier')\n",
    "\n",
    "# Evaluation Arguments\n",
    "parser.add_argument('--sample', action='store_true',\n",
    "                    help='sample when decoding for generation')\n",
    "parser.add_argument('--log_interval', type=int, default=1,\n",
    "                    help='interval to log autoencoder training results')\n",
    "\n",
    "# Other\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', dest='cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--no-cuda', dest='cuda', action='store_true',\n",
    "                    help='not using CUDA')\n",
    "parser.set_defaults(cuda=True)\n",
    "parser.add_argument('--device_id', type=str, default='0')\n",
    "\n",
    "args = parser.parse_args(['--data_path', 'data_yelp'])#Set datapath heree!!!\n",
    "if(args.outf==\"_output_glove\"):\n",
    "    args.outf = args.data_path + args.outf\n",
    "print(vars(args))\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_id\n",
    "\n",
    "# make output directory if it doesn't already exist\n",
    "if os.path.isdir(args.outf):\n",
    "    shutil.rmtree(args.outf)\n",
    "os.makedirs(args.outf)\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, \"\n",
    "              \"so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab 9599; Pruned to 9603\n",
      "Number of sentences dropped from data_yelp/valid1.txt: 0 out of 38205 total\n",
      "Number of sentences dropped from data_yelp/valid2.txt: 0 out of 25278 total\n",
      "Number of sentences dropped from data_yelp/train1.txt: 0 out of 267314 total\n",
      "Number of sentences dropped from data_yelp/train2.txt: 0 out of 176787 total\n",
      "Vocabulary Size: 9603\n",
      "382 batches\n",
      "252 batches\n",
      "4176 batches\n",
      "2762 batches\n",
      "Loaded data!\n"
     ]
    }
   ],
   "source": [
    "label_ids = {\"pos\": 1, \"neg\": 0}\n",
    "id2label = {1: \"pos\", 0: \"neg\"}\n",
    "\n",
    "# (Path to textfile, Name, Use4Vocab)\n",
    "datafiles = [(os.path.join(args.data_path, \"valid1.txt\"), \"valid1\", False),\n",
    "             (os.path.join(args.data_path, \"valid2.txt\"), \"valid2\", False),\n",
    "             (os.path.join(args.data_path, \"train1.txt\"), \"train1\", True),\n",
    "             (os.path.join(args.data_path, \"train2.txt\"), \"train2\", True)]\n",
    "vocabdict = None\n",
    "if args.load_vocab != \"\":\n",
    "    vocabdict = json.load(args.vocab)\n",
    "    vocabdict = {k: int(v) for k, v in vocabdict.items()}\n",
    "corpus = Corpus(datafiles,\n",
    "                maxlen=args.maxlen,\n",
    "                vocab_size=args.vocab_size,\n",
    "                lowercase=args.lowercase,\n",
    "                vocab=vocabdict)\n",
    "\n",
    "# dumping vocabulary\n",
    "with open('{}/vocab.json'.format(args.outf), 'w') as f:\n",
    "    json.dump(corpus.dictionary.word2idx, f) #Need to edit this if we are using GLOVE\n",
    "\n",
    "# save arguments\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(ntokens))\n",
    "args.ntokens = ntokens\n",
    "with open('{}/args.json'.format(args.outf), 'w') as f:\n",
    "    json.dump(vars(args), f)\n",
    "with open(\"{}/log.txt\".format(args.outf), 'w') as f:\n",
    "    f.write(str(vars(args)))\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "    \n",
    "eval_batch_size = 100\n",
    "test1_data = batchify(corpus.data['valid1'], eval_batch_size, shuffle=False)\n",
    "test2_data = batchify(corpus.data['valid2'], eval_batch_size, shuffle=False)\n",
    "train1_data = batchify(corpus.data['train1'], args.batch_size, shuffle=True)\n",
    "train2_data = batchify(corpus.data['train2'], args.batch_size, shuffle=True)\n",
    "\n",
    "print(\"Loaded data!\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bcolz in /data/anaconda/envs/py36/lib/python3.6/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.7 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from bcolz) (1.14.6)\n",
      "Collecting pickle\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pickle (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for pickle\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bcolz\n",
    "!pip install pickle\n",
    "import bcolz\n",
    "import pickle\n",
    "vectors = bcolz.open(f'6B.300.dat')[:]\n",
    "words = pickle.load(open(f'6B.300words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'6B.300_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}\n",
    "\n",
    "matrix_len = ntokens\n",
    "weights_matrix = np.zeros((matrix_len, 300))\n",
    "words_found = 0\n",
    "target_vocab = corpus.dictionary.word2idx\n",
    "\n",
    "for key, value in target_vocab.items():\n",
    "    try: \n",
    "        weights_matrix[value] = glove[key]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[value] = np.random.normal(scale=0.6, size=(300, ))\n",
    "weights_matrix = torch.from_numpy(weights_matrix).float().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Build model#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component to experiment with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ninput, noutput, layers, activation=nn.LeakyReLU(0.2), gpu=False):\n",
    "        #pdb.set_trace()\n",
    "        super(CNN_D, self).__init__()\n",
    "        self.ninput = ninput\n",
    "        self.noutput = noutput\n",
    "        self.conv1 = nn.Conv2d(self.ninput, 1, kernel_size=4,\\\n",
    "                               stride=1, padding=2, bias=True)\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        #pdb.set_trace()\n",
    "        # flatten before feeding into fully connected\n",
    "        self.fc1 = nn.Linear(256, noutput, bias=True)  # calculate size here,1 bias = True)\n",
    "        #256 is palce holder\n",
    "    def forward(self, x):\n",
    "        #pdb.set_trace()\n",
    "        # get last item\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        #print(x.shape)\n",
    "        #pdb.set_trace()\n",
    "        x=self.conv1(x)\n",
    "        x=self.dropout(self.activation(x))\n",
    "        x=x.view(-1)\n",
    "        #print(self.noutput,'after',x.shape)\n",
    "        logits=self.fc1(x)\n",
    "        #print(logits.shape)\n",
    "        #pdb.set_trace()\n",
    "        return logits\n",
    "# print(\"skip this pls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_D(\n",
      "  (layer1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (activation1): LeakyReLU(negative_slope=0.2)\n",
      "  (layer2): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): LeakyReLU(negative_slope=0.2)\n",
      "  (layer6): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "Seq2Seq2Decoder(\n",
      "  (embedding): Embedding(9603, 300)\n",
      "  (embedding_decoder1): Embedding(9603, 300)\n",
      "  (embedding_decoder2): Embedding(9603, 300)\n",
      "  (encoder): LSTM(300, 300, batch_first=True)\n",
      "  (decoder1): LSTM(600, 300, batch_first=True)\n",
      "  (decoder2): LSTM(600, 300, batch_first=True)\n",
      "  (linear): Linear(in_features=300, out_features=9603, bias=True)\n",
      ")\n",
      "MLP_G(\n",
      "  (layer1): Linear(in_features=32, out_features=300, bias=True)\n",
      "  (bn1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): ReLU()\n",
      "  (layer2): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): ReLU()\n",
      "  (layer7): Linear(in_features=300, out_features=300, bias=True)\n",
      ")\n",
      "MLP_D(\n",
      "  (layer1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (activation1): LeakyReLU(negative_slope=0.2)\n",
      "  (layer2): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): LeakyReLU(negative_slope=0.2)\n",
      "  (layer6): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "MLP_Classify(\n",
      "  (layer1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (activation1): ReLU()\n",
      "  (layer2): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): ReLU()\n",
      "  (layer6): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "autoencoder = Seq2Seq2Decoder(emsize=args.emsize,\\\n",
    "                              nhidden=args.nhidden,\\\n",
    "                              ntokens=ntokens,\\\n",
    "                              nlayers=args.nlayers,\\\n",
    "                              weight_matrix = weights_matrix,\\\n",
    "                              noise_r=args.noise_r,\\\n",
    "                              hidden_init=args.hidden_init,\\\n",
    "                              dropout=args.dropout,\\\n",
    "                              gpu=args.cuda)\n",
    "\n",
    "gan_gen = MLP_G(ninput=args.z_size, noutput=args.nhidden, layers=args.arch_g)\n",
    "gan_disc = MLP_D(ninput=args.nhidden, noutput=1, layers=args.arch_d)\n",
    "print(gan_disc)\n",
    "#pdb.set_trace()\n",
    "classifier = MLP_Classify(\n",
    "    ninput=args.nhidden, noutput=1, layers=args.arch_classify)\n",
    "g_factor = None\n",
    "\n",
    "print(autoencoder)\n",
    "print(gan_gen)\n",
    "print(gan_disc)\n",
    "print(classifier)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=args.lr_ae)\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\\\n",
    "                             lr=args.lr_gan_g,\\\n",
    "                             betas=(args.beta1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\\\n",
    "                             lr=args.lr_gan_d,\\\n",
    "                             betas=(args.beta1, 0.999))\n",
    "# classify\n",
    "optimizer_classify = optim.Adam(classifier.parameters(),\\\n",
    "                                lr=args.lr_classify,\\\n",
    "                                betas=(args.beta1, 0.999))\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "if args.cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    gan_gen = gan_gen.cuda()\n",
    "    gan_disc = gan_disc.cuda()\n",
    "    classifier = classifier.cuda()\n",
    "    criterion_ce = criterion_ce.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    print(\"Saving models\")\n",
    "    with open('{}/autoencoder_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(autoencoder.state_dict(), f)\n",
    "    with open('{}/gan_gen_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(gan_gen.state_dict(), f)\n",
    "    with open('{}/gan_disc_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(gan_disc.state_dict(), f)\n",
    "\n",
    "\n",
    "def train_classifier(whichclass, batch):\n",
    "    classifier.train()\n",
    "    classifier.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    labels = to_gpu(args.cuda, Variable(\n",
    "        torch.zeros(source.size(0)).fill_(whichclass - 1)))\n",
    "\n",
    "    # Train\n",
    "    code = autoencoder(0, source, lengths, noise=False,\n",
    "                       encode_only=True).detach()\n",
    "    scores = classifier(code)\n",
    "    classify_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "    classify_loss.backward()\n",
    "    optimizer_classify.step()\n",
    "    classify_loss = classify_loss.cpu().data[0]\n",
    "\n",
    "    pred = scores.data.round().squeeze(1)\n",
    "    accuracy = pred.eq(labels.data).float().mean()\n",
    "\n",
    "    return classify_loss, accuracy\n",
    "\n",
    "\n",
    "def grad_hook_cla(grad):\n",
    "    return grad * args.lambda_class\n",
    "\n",
    "\n",
    "def classifier_regularize(whichclass, batch):\n",
    "    autoencoder.train()\n",
    "    autoencoder.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "    flippedclass = abs(2 - whichclass)\n",
    "    labels = to_gpu(args.cuda, Variable(\n",
    "        torch.zeros(source.size(0)).fill_(flippedclass)))\n",
    "\n",
    "    # Train\n",
    "    code = autoencoder(0, source, lengths, noise=False, encode_only=True)\n",
    "    code.register_hook(grad_hook_cla)\n",
    "    scores = classifier(code)\n",
    "    classify_reg_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "    classify_reg_loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), args.clip)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    return classify_reg_loss\n",
    "\n",
    "\n",
    "def evaluate_autoencoder(whichdecoder, data_source, epoch):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary.word2idx)\n",
    "    all_accuracies = 0\n",
    "    bcnt = 0\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        source = to_gpu(args.cuda, Variable(source, volatile=True))\n",
    "        target = to_gpu(args.cuda, Variable(target, volatile=True))\n",
    "\n",
    "        mask = target.gt(0)\n",
    "        masked_target = target.masked_select(mask)\n",
    "        # examples x ntokens\n",
    "        output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "\n",
    "        hidden = autoencoder(0, source, lengths, noise=False, encode_only=True)\n",
    "\n",
    "        # output: batch x seq_len x ntokens\n",
    "        if whichdecoder == 1:\n",
    "            output = autoencoder(1, source, lengths, noise=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            # accuracy\n",
    "            max_vals1, max_indices1 = torch.max(masked_output, 1)\n",
    "            all_accuracies += \\\n",
    "                torch.mean(max_indices1.eq(masked_target).float()).data[0]\n",
    "\n",
    "            max_values1, max_indices1 = torch.max(output, 2)\n",
    "            max_indices2 = autoencoder.generate(2, hidden, maxlen=50)\n",
    "        else:\n",
    "            output = autoencoder(2, source, lengths, noise=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            # accuracy\n",
    "            max_vals2, max_indices2 = torch.max(masked_output, 1)\n",
    "            all_accuracies += \\\n",
    "                torch.mean(max_indices2.eq(masked_target).float()).data[0]\n",
    "\n",
    "            max_values2, max_indices2 = torch.max(output, 2)\n",
    "            max_indices1 = autoencoder.generate(1, hidden, maxlen=50)\n",
    "\n",
    "        total_loss += criterion_ce(masked_output /\n",
    "                                   args.temp, masked_target).data\n",
    "        bcnt += 1\n",
    "\n",
    "        aeoutf_from = \"{}/{}_output_decoder_{}_from.txt\".format(\n",
    "            args.outf, epoch, whichdecoder)\n",
    "        aeoutf_tran = \"{}/{}_output_decoder_{}_tran.txt\".format(\n",
    "            args.outf, epoch, whichdecoder)\n",
    "        with open(aeoutf_from, 'w') as f_from, open(aeoutf_tran, 'w') as f_trans:\n",
    "            max_indices1 = \\\n",
    "                max_indices1.view(output.size(0), -1).data.cpu().numpy()\n",
    "            max_indices2 = \\\n",
    "                max_indices2.view(output.size(0), -1).data.cpu().numpy()\n",
    "            target = target.view(output.size(0), -1).data.cpu().numpy()\n",
    "            tran_indices = max_indices2 if whichdecoder == 1 else max_indices1\n",
    "            for t, tran_idx in zip(target, tran_indices):\n",
    "                # real sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in t])\n",
    "                f_from.write(chars)\n",
    "                f_from.write(\"\\n\")\n",
    "                # transfer sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x]\n",
    "                                  for x in tran_idx])\n",
    "                f_trans.write(chars)\n",
    "                f_trans.write(\"\\n\")\n",
    "\n",
    "    return total_loss[0] / len(data_source), all_accuracies / bcnt\n",
    "\n",
    "\n",
    "def evaluate_generator(whichdecoder, noise, epoch):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # generate from fixed random noise\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = \\\n",
    "        autoencoder.generate(whichdecoder, fake_hidden,\n",
    "                             maxlen=50, sample=args.sample)\n",
    "\n",
    "    with open(\"%s/%s_generated%d.txt\" % (args.outf, epoch, whichdecoder), \"w\") as f:\n",
    "        max_indices = max_indices.data.cpu().numpy()\n",
    "        for idx in max_indices:\n",
    "            # generated sentence\n",
    "            #Use intermediate for glove here\n",
    "            words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "            # truncate sentences to first occurrence of <eos>\n",
    "            truncated_sent = []\n",
    "            for w in words:\n",
    "                if w != '<eos>':\n",
    "                    truncated_sent.append(w)\n",
    "                else:\n",
    "                    break\n",
    "            chars = \" \".join(truncated_sent)\n",
    "            f.write(chars)\n",
    "            #print(\"word length is:\", len(words))\n",
    "            #print('generated output:\\n', chars)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def train_ae(whichdecoder, batch, total_loss_ae, start_time, i):\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "\n",
    "    mask = target.gt(0)\n",
    "    masked_target = target.masked_select(mask)\n",
    "    output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "    output = autoencoder(whichdecoder, source, lengths, noise=True)\n",
    "    flat_output = output.view(-1, ntokens)\n",
    "    masked_output = flat_output.masked_select(output_mask).view(-1, ntokens)\n",
    "    loss = criterion_ce(masked_output / args.temp, masked_target)\n",
    "    loss.backward()\n",
    "\n",
    "    # `clip_grad_norm` to prevent exploding gradient in RNNs / LSTMs\n",
    "    torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), args.clip)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    total_loss_ae += loss.data\n",
    "\n",
    "    accuracy = None\n",
    "    if i % args.log_interval == 0 and i > 0:\n",
    "        probs = F.softmax(masked_output, dim=-1)\n",
    "        max_vals, max_indices = torch.max(probs, 1)\n",
    "        accuracy = torch.mean(max_indices.eq(masked_target).float()).data[0]\n",
    "        cur_loss = total_loss_ae[0] / args.log_interval\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "              'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}'\n",
    "              .format(epoch, i, len(train1_data),\n",
    "                      elapsed * 1000 / args.log_interval,\n",
    "                      cur_loss, math.exp(cur_loss), accuracy))\n",
    "\n",
    "        with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "            f.write('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}\\n'.\n",
    "                    format(epoch, i, len(train1_data),\n",
    "                           elapsed * 1000 / args.log_interval,\n",
    "                           cur_loss, math.exp(cur_loss), accuracy))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    return total_loss_ae, start_time\n",
    "\n",
    "\n",
    "def train_gan_g():\n",
    "    gan_gen.train()\n",
    "    gan_gen.zero_grad()\n",
    "\n",
    "    noise = to_gpu(args.cuda,Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "    noise.data.normal_(0, 1)\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    errG = gan_disc(fake_hidden)\n",
    "    errG.backward(one)\n",
    "    optimizer_gan_g.step()\n",
    "\n",
    "    return errG\n",
    "\n",
    "\n",
    "def grad_hook(grad):\n",
    "    return grad * args.grad_lambda\n",
    "\n",
    "\n",
    "''' Steal from https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py '''\n",
    "\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    bsz = real_data.size(0)\n",
    "    alpha = torch.rand(bsz, 1)\n",
    "    alpha = alpha.expand(bsz, real_data.size(1))  # only works for 2D XXX\n",
    "    alpha = alpha.cuda()\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\\\n",
    "                                    grad_outputs=torch.ones(\\\n",
    "                                        disc_interpolates.size()).cuda(),\\\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1)\n",
    "                        ** 2).mean() * args.gan_gp_lambda\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def train_gan_d(whichdecoder, batch):\n",
    "    gan_disc.train()\n",
    "    optimizer_gan_d.zero_grad()\n",
    "\n",
    "    # positive samples ----------------------------\n",
    "    # generate real codes\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "\n",
    "    # batch_size x nhidden\n",
    "    real_hidden = autoencoder(whichdecoder, source,lengths, noise=False, encode_only=True)\n",
    "\n",
    "    # loss / backprop\n",
    "    errD_real = gan_disc(real_hidden)\n",
    "    errD_real.backward(one)\n",
    "\n",
    "    # negative samples ----------------------------\n",
    "    # generate fake codes\n",
    "    noise = to_gpu(args.cuda,\n",
    "                Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "    noise.data.normal_(0, 1)\n",
    "\n",
    "    # loss / backprop\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    errD_fake = gan_disc(fake_hidden.detach())\n",
    "    errD_fake.backward(mone)\n",
    "\n",
    "    # gradient penalty\n",
    "    gradient_penalty = calc_gradient_penalty(gan_disc, real_hidden.data, fake_hidden.data)\n",
    "    gradient_penalty.backward()\n",
    "\n",
    "    optimizer_gan_d.step()\n",
    "    errD = -(errD_real.unsqueeze(0) - errD_fake.unsqueeze(0))\n",
    "\n",
    "    return errD, errD_real, errD_fake\n",
    "\n",
    "\n",
    "def train_gan_d_into_ae(whichdecoder, batch):\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "    real_hidden = autoencoder(whichdecoder, source, lengths, noise=False, encode_only=True)\n",
    "    real_hidden.register_hook(grad_hook)\n",
    "    errD_real = gan_disc(real_hidden)\n",
    "    errD_real.backward(mone)\n",
    "    torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), args.clip)\n",
    "\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    return errD_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:99: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:107: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:108: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:194: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:195: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][1/4176] Loss_D: 0.0517 (Loss_D_real: -0.0505 Loss_D_fake: 0.0012) Loss_G: 0.0014\n",
      "Classify loss:  0.69 | Classify accuracy: 0.414\n",
      "\n",
      "| epoch   1 |     1/ 4176 batches | ms/batch 2567.37 | loss 18.17 | ppl 77943403.90 | acc     0.10\n",
      "| epoch   1 |     1/ 4176 batches | ms/batch 42.85 | loss 18.11 | ppl 73438962.68 | acc     0.10\n",
      "[1/25][2/4176] Loss_D: 0.1501 (Loss_D_real: -0.1509 Loss_D_fake: -0.0007) Loss_G: -0.0007\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "| epoch   1 |     2/ 4176 batches | ms/batch 2321.51 | loss  8.76 | ppl  6375.10 | acc     0.10\n",
      "| epoch   1 |     2/ 4176 batches | ms/batch 45.65 | loss  8.66 | ppl  5744.85 | acc     0.11\n",
      "[1/25][3/4176] Loss_D: 0.2844 (Loss_D_real: -0.2951 Loss_D_fake: -0.0107) Loss_G: -0.0233\n",
      "Classify loss:  0.69 | Classify accuracy: 0.461\n",
      "\n",
      "| epoch   1 |     3/ 4176 batches | ms/batch 2345.89 | loss  8.40 | ppl  4454.69 | acc     0.10\n",
      "| epoch   1 |     3/ 4176 batches | ms/batch 44.43 | loss  8.27 | ppl  3897.03 | acc     0.09\n",
      "[1/25][4/4176] Loss_D: 0.4591 (Loss_D_real: -0.4604 Loss_D_fake: -0.0013) Loss_G: 0.0030\n",
      "Classify loss:  0.69 | Classify accuracy: 0.445\n",
      "\n",
      "| epoch   1 |     4/ 4176 batches | ms/batch 2310.80 | loss  7.79 | ppl  2420.94 | acc     0.11\n",
      "| epoch   1 |     4/ 4176 batches | ms/batch 42.35 | loss  7.84 | ppl  2538.26 | acc     0.09\n",
      "[1/25][5/4176] Loss_D: 0.6437 (Loss_D_real: -0.6446 Loss_D_fake: -0.0009) Loss_G: -0.0429\n",
      "Classify loss:  0.69 | Classify accuracy: 0.477\n",
      "\n",
      "| epoch   1 |     5/ 4176 batches | ms/batch 2385.23 | loss  7.41 | ppl  1652.55 | acc     0.19\n",
      "| epoch   1 |     5/ 4176 batches | ms/batch 41.90 | loss  7.65 | ppl  2110.83 | acc     0.11\n",
      "[1/25][6/4176] Loss_D: 0.9475 (Loss_D_real: -0.8553 Loss_D_fake: 0.0923) Loss_G: 0.0911\n",
      "Classify loss:  0.69 | Classify accuracy: 0.586\n",
      "\n",
      "| epoch   1 |     6/ 4176 batches | ms/batch 2408.05 | loss  7.84 | ppl  2549.51 | acc     0.10\n",
      "| epoch   1 |     6/ 4176 batches | ms/batch 46.57 | loss  7.78 | ppl  2403.66 | acc     0.08\n",
      "[1/25][7/4176] Loss_D: 1.2719 (Loss_D_real: -1.1304 Loss_D_fake: 0.1416) Loss_G: 0.1726\n",
      "Classify loss:  0.69 | Classify accuracy: 0.586\n",
      "\n",
      "| epoch   1 |     7/ 4176 batches | ms/batch 2366.33 | loss  7.52 | ppl  1838.22 | acc     0.13\n",
      "| epoch   1 |     7/ 4176 batches | ms/batch 45.50 | loss  7.61 | ppl  2020.01 | acc     0.09\n",
      "[1/25][8/4176] Loss_D: 1.2909 (Loss_D_real: -1.0331 Loss_D_fake: 0.2578) Loss_G: 0.3200\n",
      "Classify loss:  0.69 | Classify accuracy: 0.578\n",
      "\n",
      "| epoch   1 |     8/ 4176 batches | ms/batch 2351.30 | loss  7.11 | ppl  1219.54 | acc     0.10\n",
      "| epoch   1 |     8/ 4176 batches | ms/batch 44.31 | loss  7.49 | ppl  1788.23 | acc     0.09\n",
      "[1/25][9/4176] Loss_D: 1.1600 (Loss_D_real: -0.9664 Loss_D_fake: 0.1936) Loss_G: 0.2541\n",
      "Classify loss:  0.69 | Classify accuracy: 0.617\n",
      "\n",
      "| epoch   1 |     9/ 4176 batches | ms/batch 2313.15 | loss  6.90 | ppl   994.44 | acc     0.13\n",
      "| epoch   1 |     9/ 4176 batches | ms/batch 45.18 | loss  7.16 | ppl  1293.17 | acc     0.09\n",
      "[1/25][10/4176] Loss_D: 1.4189 (Loss_D_real: -0.9584 Loss_D_fake: 0.4606) Loss_G: 0.4010\n",
      "Classify loss:  0.69 | Classify accuracy: 0.562\n",
      "\n",
      "| epoch   1 |    10/ 4176 batches | ms/batch 2323.38 | loss  6.96 | ppl  1057.21 | acc     0.14\n",
      "| epoch   1 |    10/ 4176 batches | ms/batch 46.83 | loss  7.76 | ppl  2342.62 | acc     0.10\n",
      "[1/25][11/4176] Loss_D: 1.1990 (Loss_D_real: -1.0639 Loss_D_fake: 0.1351) Loss_G: 0.1603\n",
      "Classify loss:  0.69 | Classify accuracy: 0.578\n",
      "\n",
      "| epoch   1 |    11/ 4176 batches | ms/batch 2234.97 | loss  6.68 | ppl   796.52 | acc     0.09\n",
      "| epoch   1 |    11/ 4176 batches | ms/batch 43.71 | loss  7.27 | ppl  1438.31 | acc     0.11\n",
      "[1/25][12/4176] Loss_D: 1.0002 (Loss_D_real: -0.9031 Loss_D_fake: 0.0970) Loss_G: 0.1340\n",
      "Classify loss:  0.69 | Classify accuracy: 0.656\n",
      "\n",
      "| epoch   1 |    12/ 4176 batches | ms/batch 2338.74 | loss  7.47 | ppl  1753.88 | acc     0.12\n",
      "| epoch   1 |    12/ 4176 batches | ms/batch 44.11 | loss  6.92 | ppl  1016.05 | acc     0.09\n",
      "[1/25][13/4176] Loss_D: 1.7315 (Loss_D_real: -1.0410 Loss_D_fake: 0.6906) Loss_G: 0.7271\n",
      "Classify loss:  0.69 | Classify accuracy: 0.656\n",
      "\n",
      "| epoch   1 |    13/ 4176 batches | ms/batch 2337.51 | loss  6.52 | ppl   680.86 | acc     0.20\n",
      "| epoch   1 |    13/ 4176 batches | ms/batch 43.35 | loss  7.39 | ppl  1626.42 | acc     0.11\n",
      "[1/25][14/4176] Loss_D: 1.1921 (Loss_D_real: -1.0461 Loss_D_fake: 0.1460) Loss_G: 0.1780\n",
      "Classify loss:  0.69 | Classify accuracy: 0.641\n",
      "\n",
      "| epoch   1 |    14/ 4176 batches | ms/batch 2351.57 | loss  6.60 | ppl   732.56 | acc     0.08\n",
      "| epoch   1 |    14/ 4176 batches | ms/batch 45.68 | loss  7.01 | ppl  1108.59 | acc     0.12\n",
      "[1/25][15/4176] Loss_D: 1.5537 (Loss_D_real: -0.7493 Loss_D_fake: 0.8044) Loss_G: 0.8290\n",
      "Classify loss:  0.69 | Classify accuracy: 0.625\n",
      "\n",
      "| epoch   1 |    15/ 4176 batches | ms/batch 2381.44 | loss  7.05 | ppl  1149.28 | acc     0.12\n",
      "| epoch   1 |    15/ 4176 batches | ms/batch 44.88 | loss  6.78 | ppl   880.90 | acc     0.10\n",
      "[1/25][16/4176] Loss_D: 1.9984 (Loss_D_real: -0.6610 Loss_D_fake: 1.3373) Loss_G: 1.2683\n",
      "Classify loss:  0.69 | Classify accuracy: 0.656\n",
      "\n",
      "| epoch   1 |    16/ 4176 batches | ms/batch 2366.29 | loss  6.58 | ppl   720.41 | acc     0.14\n",
      "| epoch   1 |    16/ 4176 batches | ms/batch 42.76 | loss  7.47 | ppl  1753.96 | acc     0.12\n",
      "[1/25][17/4176] Loss_D: 1.8587 (Loss_D_real: -0.6801 Loss_D_fake: 1.1786) Loss_G: 1.1954\n",
      "Classify loss:  0.69 | Classify accuracy: 0.609\n",
      "\n",
      "| epoch   1 |    17/ 4176 batches | ms/batch 2388.40 | loss  6.51 | ppl   668.76 | acc     0.10\n",
      "| epoch   1 |    17/ 4176 batches | ms/batch 44.59 | loss  6.72 | ppl   826.99 | acc     0.11\n",
      "[1/25][18/4176] Loss_D: 1.7669 (Loss_D_real: -0.7152 Loss_D_fake: 1.0517) Loss_G: 1.1794\n",
      "Classify loss:  0.69 | Classify accuracy: 0.648\n",
      "\n",
      "| epoch   1 |    18/ 4176 batches | ms/batch 2378.04 | loss  6.72 | ppl   828.56 | acc     0.12\n",
      "| epoch   1 |    18/ 4176 batches | ms/batch 43.20 | loss  6.77 | ppl   871.89 | acc     0.11\n",
      "[1/25][19/4176] Loss_D: 1.5049 (Loss_D_real: -0.8773 Loss_D_fake: 0.6275) Loss_G: 0.7055\n",
      "Classify loss:  0.69 | Classify accuracy: 0.742\n",
      "\n",
      "| epoch   1 |    19/ 4176 batches | ms/batch 2355.00 | loss  6.48 | ppl   654.21 | acc     0.16\n",
      "| epoch   1 |    19/ 4176 batches | ms/batch 44.40 | loss  6.98 | ppl  1076.33 | acc     0.11\n",
      "[1/25][20/4176] Loss_D: 1.4409 (Loss_D_real: -0.9029 Loss_D_fake: 0.5379) Loss_G: 0.6550\n",
      "Classify loss:  0.69 | Classify accuracy: 0.680\n",
      "\n",
      "| epoch   1 |    20/ 4176 batches | ms/batch 2367.62 | loss  6.07 | ppl   431.44 | acc     0.11\n",
      "| epoch   1 |    20/ 4176 batches | ms/batch 42.37 | loss  6.67 | ppl   789.10 | acc     0.12\n",
      "[1/25][21/4176] Loss_D: 1.9603 (Loss_D_real: -1.1015 Loss_D_fake: 0.8588) Loss_G: 0.7096\n",
      "Classify loss:  0.69 | Classify accuracy: 0.695\n",
      "\n",
      "| epoch   1 |    21/ 4176 batches | ms/batch 2332.21 | loss  7.15 | ppl  1269.83 | acc     0.11\n",
      "| epoch   1 |    21/ 4176 batches | ms/batch 45.88 | loss  6.65 | ppl   775.38 | acc     0.10\n",
      "[1/25][22/4176] Loss_D: 1.9143 (Loss_D_real: -1.2825 Loss_D_fake: 0.6318) Loss_G: 0.7180\n",
      "Classify loss:  0.69 | Classify accuracy: 0.641\n",
      "\n",
      "| epoch   1 |    22/ 4176 batches | ms/batch 2354.04 | loss  6.29 | ppl   538.07 | acc     0.12\n",
      "| epoch   1 |    22/ 4176 batches | ms/batch 44.13 | loss  6.55 | ppl   700.23 | acc     0.11\n",
      "[1/25][23/4176] Loss_D: 1.9810 (Loss_D_real: -1.3156 Loss_D_fake: 0.6655) Loss_G: 0.8497\n",
      "Classify loss:  0.69 | Classify accuracy: 0.609\n",
      "\n",
      "| epoch   1 |    23/ 4176 batches | ms/batch 2327.25 | loss  6.44 | ppl   627.00 | acc     0.09\n",
      "| epoch   1 |    23/ 4176 batches | ms/batch 42.61 | loss  6.82 | ppl   917.37 | acc     0.05\n",
      "[1/25][24/4176] Loss_D: 2.2295 (Loss_D_real: -1.8864 Loss_D_fake: 0.3431) Loss_G: 0.1859\n",
      "Classify loss:  0.69 | Classify accuracy: 0.547\n",
      "\n",
      "| epoch   1 |    24/ 4176 batches | ms/batch 2341.63 | loss  6.19 | ppl   488.07 | acc     0.12\n",
      "| epoch   1 |    24/ 4176 batches | ms/batch 43.01 | loss  6.70 | ppl   809.14 | acc     0.10\n",
      "[1/25][25/4176] Loss_D: 2.3021 (Loss_D_real: -2.0286 Loss_D_fake: 0.2735) Loss_G: 0.0760\n",
      "Classify loss:  0.69 | Classify accuracy: 0.672\n",
      "\n",
      "| epoch   1 |    25/ 4176 batches | ms/batch 2266.39 | loss  6.06 | ppl   426.49 | acc     0.10\n",
      "| epoch   1 |    25/ 4176 batches | ms/batch 44.53 | loss  6.73 | ppl   834.99 | acc     0.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][26/4176] Loss_D: 2.1238 (Loss_D_real: -1.9877 Loss_D_fake: 0.1360) Loss_G: 0.3691\n",
      "Classify loss:  0.69 | Classify accuracy: 0.672\n",
      "\n",
      "| epoch   1 |    26/ 4176 batches | ms/batch 2352.41 | loss  5.89 | ppl   360.61 | acc     0.10\n",
      "| epoch   1 |    26/ 4176 batches | ms/batch 45.65 | loss  6.67 | ppl   785.74 | acc     0.12\n",
      "[1/25][27/4176] Loss_D: 2.7255 (Loss_D_real: -2.0016 Loss_D_fake: 0.7239) Loss_G: 0.7496\n",
      "Classify loss:  0.69 | Classify accuracy: 0.609\n",
      "\n",
      "| epoch   1 |    27/ 4176 batches | ms/batch 2373.63 | loss  6.45 | ppl   635.57 | acc     0.13\n",
      "| epoch   1 |    27/ 4176 batches | ms/batch 42.58 | loss  6.27 | ppl   529.08 | acc     0.10\n",
      "[1/25][28/4176] Loss_D: 1.6480 (Loss_D_real: -2.0675 Loss_D_fake: -0.4195) Loss_G: -0.1118\n",
      "Classify loss:  0.69 | Classify accuracy: 0.648\n",
      "\n",
      "| epoch   1 |    28/ 4176 batches | ms/batch 2306.63 | loss  6.05 | ppl   423.14 | acc     0.15\n",
      "| epoch   1 |    28/ 4176 batches | ms/batch 46.41 | loss  7.00 | ppl  1096.61 | acc     0.10\n",
      "[1/25][29/4176] Loss_D: 2.4940 (Loss_D_real: -1.4488 Loss_D_fake: 1.0452) Loss_G: 0.9921\n",
      "Classify loss:  0.69 | Classify accuracy: 0.625\n",
      "\n",
      "| epoch   1 |    29/ 4176 batches | ms/batch 2351.20 | loss  5.80 | ppl   329.38 | acc     0.10\n",
      "| epoch   1 |    29/ 4176 batches | ms/batch 42.82 | loss  6.33 | ppl   563.01 | acc     0.12\n",
      "[1/25][30/4176] Loss_D: 2.6104 (Loss_D_real: -0.9288 Loss_D_fake: 1.6816) Loss_G: 1.4011\n",
      "Classify loss:  0.69 | Classify accuracy: 0.664\n",
      "\n",
      "| epoch   1 |    30/ 4176 batches | ms/batch 2316.08 | loss  6.41 | ppl   609.70 | acc     0.12\n",
      "| epoch   1 |    30/ 4176 batches | ms/batch 42.99 | loss  6.48 | ppl   648.75 | acc     0.10\n",
      "[1/25][31/4176] Loss_D: 2.4327 (Loss_D_real: -0.9291 Loss_D_fake: 1.5035) Loss_G: 1.8370\n",
      "Classify loss:  0.69 | Classify accuracy: 0.695\n",
      "\n",
      "| epoch   1 |    31/ 4176 batches | ms/batch 2360.85 | loss  5.95 | ppl   382.17 | acc     0.17\n",
      "| epoch   1 |    31/ 4176 batches | ms/batch 45.78 | loss  6.43 | ppl   622.98 | acc     0.10\n",
      "[1/25][32/4176] Loss_D: 2.6473 (Loss_D_real: -0.8756 Loss_D_fake: 1.7717) Loss_G: 2.0730\n",
      "Classify loss:  0.69 | Classify accuracy: 0.602\n",
      "\n",
      "| epoch   1 |    32/ 4176 batches | ms/batch 2363.69 | loss  5.88 | ppl   356.57 | acc     0.19\n",
      "| epoch   1 |    32/ 4176 batches | ms/batch 46.62 | loss  6.44 | ppl   626.90 | acc     0.18\n",
      "[1/25][33/4176] Loss_D: 2.9423 (Loss_D_real: -0.5078 Loss_D_fake: 2.4345) Loss_G: 2.4467\n",
      "Classify loss:  0.69 | Classify accuracy: 0.570\n",
      "\n",
      "| epoch   1 |    33/ 4176 batches | ms/batch 2350.76 | loss  6.66 | ppl   780.50 | acc     0.12\n",
      "| epoch   1 |    33/ 4176 batches | ms/batch 45.70 | loss  6.39 | ppl   596.67 | acc     0.09\n",
      "[1/25][34/4176] Loss_D: 3.1879 (Loss_D_real: -0.5876 Loss_D_fake: 2.6002) Loss_G: 2.4148\n",
      "Classify loss:  0.69 | Classify accuracy: 0.508\n",
      "\n",
      "| epoch   1 |    34/ 4176 batches | ms/batch 2406.93 | loss  6.14 | ppl   462.13 | acc     0.13\n",
      "| epoch   1 |    34/ 4176 batches | ms/batch 43.52 | loss  6.45 | ppl   635.22 | acc     0.11\n",
      "[1/25][35/4176] Loss_D: 3.0845 (Loss_D_real: -0.6606 Loss_D_fake: 2.4239) Loss_G: 2.4166\n",
      "Classify loss:  0.69 | Classify accuracy: 0.531\n",
      "\n",
      "| epoch   1 |    35/ 4176 batches | ms/batch 2348.40 | loss  5.93 | ppl   377.68 | acc     0.08\n",
      "| epoch   1 |    35/ 4176 batches | ms/batch 43.17 | loss  6.44 | ppl   628.71 | acc     0.13\n",
      "[1/25][36/4176] Loss_D: 3.3477 (Loss_D_real: -0.1803 Loss_D_fake: 3.1674) Loss_G: 3.0817\n",
      "Classify loss:  0.69 | Classify accuracy: 0.539\n",
      "\n",
      "| epoch   1 |    36/ 4176 batches | ms/batch 2332.88 | loss  6.07 | ppl   432.29 | acc     0.13\n",
      "| epoch   1 |    36/ 4176 batches | ms/batch 44.52 | loss  6.10 | ppl   443.65 | acc     0.20\n",
      "[1/25][37/4176] Loss_D: 1.3114 (Loss_D_real: 1.0127 Loss_D_fake: 2.3242) Loss_G: 2.2399\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "| epoch   1 |    37/ 4176 batches | ms/batch 2349.68 | loss  5.81 | ppl   332.20 | acc     0.20\n",
      "| epoch   1 |    37/ 4176 batches | ms/batch 43.66 | loss  6.60 | ppl   732.55 | acc     0.10\n",
      "[1/25][38/4176] Loss_D: 3.0586 (Loss_D_real: -0.1985 Loss_D_fake: 2.8600) Loss_G: 3.0564\n",
      "Classify loss:  0.69 | Classify accuracy: 0.555\n",
      "\n",
      "| epoch   1 |    38/ 4176 batches | ms/batch 2248.14 | loss  5.60 | ppl   271.45 | acc     0.15\n",
      "| epoch   1 |    38/ 4176 batches | ms/batch 44.28 | loss  6.34 | ppl   567.37 | acc     0.17\n",
      "[1/25][39/4176] Loss_D: 2.8371 (Loss_D_real: -0.6391 Loss_D_fake: 2.1980) Loss_G: 2.0876\n",
      "Classify loss:  0.69 | Classify accuracy: 0.547\n",
      "\n",
      "| epoch   1 |    39/ 4176 batches | ms/batch 2351.51 | loss  6.28 | ppl   534.94 | acc     0.13\n",
      "| epoch   1 |    39/ 4176 batches | ms/batch 43.95 | loss  6.19 | ppl   490.29 | acc     0.09\n",
      "[1/25][40/4176] Loss_D: 2.5675 (Loss_D_real: -0.2297 Loss_D_fake: 2.3378) Loss_G: 2.3863\n",
      "Classify loss:  0.69 | Classify accuracy: 0.523\n",
      "\n",
      "| epoch   1 |    40/ 4176 batches | ms/batch 2311.57 | loss  5.90 | ppl   366.25 | acc     0.16\n",
      "| epoch   1 |    40/ 4176 batches | ms/batch 43.65 | loss  6.43 | ppl   621.90 | acc     0.11\n",
      "[1/25][41/4176] Loss_D: 2.8321 (Loss_D_real: -0.3789 Loss_D_fake: 2.4532) Loss_G: 2.5268\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "| epoch   1 |    41/ 4176 batches | ms/batch 2343.54 | loss  5.54 | ppl   254.35 | acc     0.21\n",
      "| epoch   1 |    41/ 4176 batches | ms/batch 45.23 | loss  6.18 | ppl   484.25 | acc     0.15\n",
      "[1/25][42/4176] Loss_D: 3.3283 (Loss_D_real: -0.4793 Loss_D_fake: 2.8490) Loss_G: 2.9943\n",
      "Classify loss:  0.69 | Classify accuracy: 0.609\n",
      "\n",
      "| epoch   1 |    42/ 4176 batches | ms/batch 2326.60 | loss  5.68 | ppl   293.49 | acc     0.14\n",
      "| epoch   1 |    42/ 4176 batches | ms/batch 45.40 | loss  5.95 | ppl   382.26 | acc     0.13\n",
      "[1/25][43/4176] Loss_D: 3.2945 (Loss_D_real: -0.4013 Loss_D_fake: 2.8932) Loss_G: 2.9984\n",
      "Classify loss:  0.69 | Classify accuracy: 0.609\n",
      "\n",
      "| epoch   1 |    43/ 4176 batches | ms/batch 2393.85 | loss  5.78 | ppl   323.51 | acc     0.16\n",
      "| epoch   1 |    43/ 4176 batches | ms/batch 43.65 | loss  6.57 | ppl   713.15 | acc     0.11\n",
      "[1/25][44/4176] Loss_D: 3.9513 (Loss_D_real: -1.1909 Loss_D_fake: 2.7603) Loss_G: 2.8965\n",
      "Classify loss:  0.69 | Classify accuracy: 0.539\n",
      "\n",
      "| epoch   1 |    44/ 4176 batches | ms/batch 2319.98 | loss  5.64 | ppl   282.51 | acc     0.22\n",
      "| epoch   1 |    44/ 4176 batches | ms/batch 44.45 | loss  6.39 | ppl   595.30 | acc     0.13\n",
      "[1/25][45/4176] Loss_D: 3.1004 (Loss_D_real: -0.4139 Loss_D_fake: 2.6865) Loss_G: 2.8373\n",
      "Classify loss:  0.69 | Classify accuracy: 0.555\n",
      "\n",
      "| epoch   1 |    45/ 4176 batches | ms/batch 2363.02 | loss  6.03 | ppl   415.23 | acc     0.12\n",
      "| epoch   1 |    45/ 4176 batches | ms/batch 46.64 | loss  5.97 | ppl   389.56 | acc     0.19\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "    f.write('Training...\\n')\n",
    "\n",
    "# schedule of increasing GAN training loops\n",
    "if args.niters_gan_schedule != \"\":\n",
    "    gan_schedule = [int(x) for x in args.niters_gan_schedule.split(\"-\")]\n",
    "else:\n",
    "    gan_schedule = []\n",
    "niter_gan = 25\n",
    "\n",
    "fixed_noise = to_gpu(args.cuda,Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "fixed_noise.data.normal_(0, 1)\n",
    "one = Variable(torch.tensor(1.0).cuda())#to_gpu(args.cuda, torch.FloatTensor([1]))\n",
    "mone = Variable(torch.tensor(-1.0).cuda())#one * -1\n",
    "#one = Variable(one, requires_grad=True).cuda()#torch.tensor(1.0, dtype=torch.float64,device=torch.device('cuda:0'))\n",
    "#mone = #Variable(mone, requires_grad=True).cuda()\n",
    "#mone = torch.tensor(-1.0, dtype=torch.float64,device=torch.device('cuda:0'))\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    # update gan training schedule\n",
    "    if epoch in gan_schedule:\n",
    "        niter_gan += 1\n",
    "        print(\"GAN training loop schedule increased to {}\".format(niter_gan))\n",
    "        with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "            f.write(\"GAN training loop schedule increased to {}\\n\".\n",
    "                    format(niter_gan))\n",
    "\n",
    "    total_loss_ae1 = 0\n",
    "    total_loss_ae2 = 0\n",
    "    classify_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "    start_time = time.time()\n",
    "    niter = 0\n",
    "    niter_global = 1\n",
    "\n",
    "    # loop through all batches in training data\n",
    "    while niter < len(train1_data) and niter < len(train2_data):\n",
    "\n",
    "        # train autoencoder ----------------------------\n",
    "        for i in range(args.niters_ae):\n",
    "            if niter == len(train1_data):\n",
    "                break  # end of epoch\n",
    "            total_loss_ae1, start_time = train_ae(1, train1_data[niter], total_loss_ae1, start_time, niter)\n",
    "            total_loss_ae2, _ = train_ae(2, train2_data[niter], total_loss_ae2, start_time, niter)\n",
    "\n",
    "            # train classifier ----------------------------\n",
    "            classify_loss1, classify_acc1 = train_classifier(\n",
    "                1, train1_data[niter])\n",
    "            classify_loss2, classify_acc2 = train_classifier(\n",
    "                2, train2_data[niter])\n",
    "            classify_loss = (classify_loss1 + classify_loss2) / 2\n",
    "            classify_acc = (classify_acc1 + classify_acc2) / 2\n",
    "            # reverse to autoencoder\n",
    "            classifier_regularize(1, train1_data[niter])\n",
    "            classifier_regularize(2, train2_data[niter])\n",
    "\n",
    "            niter += 1\n",
    "\n",
    "        # train gan ----------------------------------\n",
    "        for k in range(niter_gan):\n",
    "\n",
    "            # train discriminator/critic\n",
    "            for i in range(args.niters_gan_d):\n",
    "                # feed a seen sample within this epoch; good for early training\n",
    "                if i % 2 == 0:\n",
    "                    batch = train1_data[\n",
    "                        random.randint(0, len(train1_data) - 1)]\n",
    "                    whichdecoder = 1\n",
    "                else:\n",
    "                    batch = train2_data[\n",
    "                        random.randint(0, len(train2_data) - 1)]\n",
    "                    whichdecoder = 2\n",
    "                errD, errD_real, errD_fake = train_gan_d(whichdecoder, batch)\n",
    "\n",
    "            # train generator\n",
    "            for i in range(args.niters_gan_g):\n",
    "                errG = train_gan_g()\n",
    "\n",
    "            # train autoencoder from d\n",
    "            for i in range(args.niters_gan_ae):\n",
    "                if i % 2 == 0:\n",
    "                    batch = train1_data[\n",
    "                        random.randint(0, len(train1_data) - 1)]\n",
    "                    whichdecoder = 1\n",
    "                else:\n",
    "                    batch = train2_data[\n",
    "                        random.randint(0, len(train2_data) - 1)]\n",
    "                    whichdecoder = 2\n",
    "                errD_= train_gan_d_into_ae(whichdecoder, batch)\n",
    "\n",
    "        niter_global += 1\n",
    "        if niter_global % 100 >= 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f (Loss_D_real: %.4f '\n",
    "                  'Loss_D_fake: %.4f) Loss_G: %.4f'\n",
    "                  % (epoch, args.epochs, niter, len(train1_data),\n",
    "                     errD.data[0], errD_real.data[0],\n",
    "                     errD_fake.data[0], errG.data[0]))\n",
    "            print(\"Classify loss: {:5.2f} | Classify accuracy: {:3.3f}\\n\".format(\n",
    "                classify_loss, classify_acc))\n",
    "            with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "                f.write('[%d/%d][%d/%d] Loss_D: %.4f (Loss_D_real: %.4f '\n",
    "                        'Loss_D_fake: %.4f) Loss_G: %.4f\\n'\n",
    "                        % (epoch, args.epochs, niter, len(train1_data),\n",
    "                           errD.data[0], errD_real.data[0],\n",
    "                           errD_fake.data[0], errG.data[0]))\n",
    "                f.write(\"Classify loss: {:5.2f} | Classify accuracy: {:3.3f}\\n\".format(\n",
    "                        classify_loss, classify_acc))\n",
    "\n",
    "            # exponentially decaying noise on autoencoder\n",
    "            autoencoder.noise_r = \\\n",
    "                autoencoder.noise_r * args.noise_anneal\n",
    "\n",
    "    # end of epoch ----------------------------\n",
    "    # evaluation\n",
    "    test_loss, accuracy = evaluate_autoencoder(1, test1_data[:1000], epoch)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "          'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "          format(epoch, (time.time() - epoch_start_time),\n",
    "                 test_loss, math.exp(test_loss), accuracy))\n",
    "    print('-' * 89)\n",
    "    with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "                ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "                format(epoch, (time.time() - epoch_start_time),\n",
    "                       test_loss, math.exp(test_loss), accuracy))\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n')\n",
    "\n",
    "    test_loss, accuracy = evaluate_autoencoder(2, test2_data[:1000], epoch)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "          'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "          format(epoch, (time.time() - epoch_start_time),\n",
    "                 test_loss, math.exp(test_loss), accuracy))\n",
    "    print('-' * 89)\n",
    "    with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "                ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "                format(epoch, (time.time() - epoch_start_time),\n",
    "                       test_loss, math.exp(test_loss), accuracy))\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n')\n",
    "\n",
    "    evaluate_generator(1, fixed_noise, \"end_of_epoch_{}\".format(epoch))\n",
    "    evaluate_generator(2, fixed_noise, \"end_of_epoch_{}\".format(epoch))\n",
    "\n",
    "    # shuffle between epochs\n",
    "    train1_data = batchify(\n",
    "        corpus.data['train1'], args.batch_size, shuffle=True)\n",
    "    train2_data = batchify(\n",
    "        corpus.data['train2'], args.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_loss, accuracy = evaluate_autoencoder(1, test1_data, epoch + 1)\n",
    "print('-' * 89)\n",
    "print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "      'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "      format(epoch, (time.time() - epoch_start_time),\n",
    "             test_loss, math.exp(test_loss), accuracy))\n",
    "print('-' * 89)\n",
    "with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "            ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "            format(epoch, (time.time() - epoch_start_time),\n",
    "                   test_loss, math.exp(test_loss), accuracy))\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n')\n",
    "\n",
    "test_loss, accuracy = evaluate_autoencoder(2, test2_data, epoch + 1)\n",
    "print('-' * 89)\n",
    "print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "      'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "      format(epoch, (time.time() - epoch_start_time),\n",
    "             test_loss, math.exp(test_loss), accuracy))\n",
    "print('-' * 89)\n",
    "with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "            ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "            format(epoch, (time.time() - epoch_start_time),\n",
    "                   test_loss, math.exp(test_loss), accuracy))\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
