{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, Corpus, batchify\n",
    "from models1 import Seq2Seq2Decoder\n",
    "from models3 import Seq2Seq, Seq2Seq, MLP_G, MLP_Classify, CNN_D\n",
    "import shutil\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='ARAE for Yelp transfer')\n",
    "# Path Arguments\n",
    "parser.add_argument('--data_path', type=str, default=\"data_fr\",\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--outf', type=str, default=\"_output\",\n",
    "                    help='output directory name')\n",
    "parser.add_argument('--load_vocab', type=str, default=\"\",\n",
    "                    help='path to load vocabulary from')\n",
    "\n",
    "# Data Processing Arguments\n",
    "parser.add_argument('--vocab_size', type=int, default=30000,\n",
    "                    help='cut vocabulary down to this size '\n",
    "                         '(most frequently seen words in train)')\n",
    "parser.add_argument('--maxlen', type=int, default=25,\n",
    "                    help='maximum sentence length')\n",
    "parser.add_argument('--lowercase', dest='lowercase', action='store_true',\n",
    "                    help='lowercase all text')\n",
    "parser.add_argument('--no-lowercase', dest='lowercase', action='store_true',\n",
    "                    help='not lowercase all text')\n",
    "parser.set_defaults(lowercase=True)\n",
    "\n",
    "# Model Arguments\n",
    "# changed\n",
    "parser.add_argument('--emsize', type=int, default=256,\n",
    "                    help='size of word embeddings')\n",
    "# changed\n",
    "parser.add_argument('--nhidden', type=int, default=256,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=1,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--noise_r', type=float, default=0.1,\n",
    "                    help='stdev of noise for autoencoder (regularizer)')\n",
    "parser.add_argument('--noise_anneal', type=float, default=0.9995,\n",
    "                    help='anneal noise_r exponentially by this'\n",
    "                         'every 100 iterations')\n",
    "parser.add_argument('--hidden_init', action='store_true',\n",
    "                    help=\"initialize decoder hidden state with encoder's\")\n",
    "parser.add_argument('--arch_g', type=str, default='256-256',\n",
    "                    help='generator architecture (MLP)')\n",
    "parser.add_argument('--arch_d', type=str, default='256-256',\n",
    "                    help='critic/discriminator architecture (MLP)')\n",
    "parser.add_argument('--arch_classify', type=str, default='256-256',\n",
    "                    help='classifier architecture')\n",
    "parser.add_argument('--z_size', type=int, default=32,\n",
    "                    help='dimension of random noise z to feed into generator')\n",
    "parser.add_argument('--temp', type=float, default=1,\n",
    "                    help='softmax temperature (lower --> more discrete)')\n",
    "parser.add_argument('--dropout', type=float, default=0.0,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "\n",
    "# Training Arguments\n",
    "epoch_num = 100\n",
    "parser.add_argument('--epochs', type=int, default=epoch_num, #changed from 25\n",
    "                    help='maximum number of epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--niters_ae', type=int, default=1,\n",
    "                    help='number of autoencoder iterations in training')\n",
    "parser.add_argument('--niters_gan_d', type=int, default=5,\n",
    "                    help='number of discriminator iterations in training')\n",
    "parser.add_argument('--niters_gan_g', type=int, default=1,\n",
    "                    help='number of generator iterations in training')\n",
    "parser.add_argument('--niters_gan_ae', type=int, default=1,\n",
    "                    help='number of gan-into-ae iterations in training')\n",
    "parser.add_argument('--niters_gan_schedule', type=str, default='',\n",
    "                    help='epoch counts to increase number of GAN training '\n",
    "                         ' iterations (increment by 1 each time)')\n",
    "parser.add_argument('--lr_ae', type=float, default=1,\n",
    "                    help='autoencoder learning rate')\n",
    "parser.add_argument('--lr_gan_g', type=float, default=1e-04,\n",
    "                    help='generator learning rate')\n",
    "parser.add_argument('--lr_gan_d', type=float, default=1e-04,\n",
    "                    help='critic/discriminator learning rate')\n",
    "parser.add_argument('--lr_classify', type=float, default=1e-04,\n",
    "                    help='classifier learning rate')\n",
    "parser.add_argument('--beta1', type=float, default=0.5,\n",
    "                    help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--clip', type=float, default=1,\n",
    "                    help='gradient clipping, max norm')\n",
    "parser.add_argument('--gan_gp_lambda', type=float, default=0.1,\n",
    "                    help='WGAN GP penalty lambda')\n",
    "parser.add_argument('--grad_lambda', type=float, default=0.01,\n",
    "                    help='WGAN into AE lambda')\n",
    "parser.add_argument('--lambda_class', type=float, default=1,\n",
    "                    help='lambda on classifier')\n",
    "\n",
    "# Evaluation Arguments\n",
    "parser.add_argument('--sample', action='store_true',\n",
    "                    help='sample when decoding for generation')\n",
    "parser.add_argument('--log_interval', type=int, default=200,\n",
    "                    help='interval to log autoencoder training results')\n",
    "\n",
    "# # Other\n",
    "# parser.add_argument('--seed', type=int, default=1111,\n",
    "#                     help='random seed')\n",
    "# parser.add_argument('--cuda', dest='cuda', action='store_true',\n",
    "#                     help='use CUDA')\n",
    "# parser.add_argument('--no-cuda', dest='cuda', action='store_true',\n",
    "#                     help='not using CUDA')\n",
    "# parser.set_defaults(cuda=True)\n",
    "# parser.add_argument('--device_id', type=str, default='0')\n",
    "args = parser.parse_args(['--data_path', 'data_fr'])\n",
    "# if(args.outf == \"_output\"):\n",
    "#     args.outf = args.data_path + args.outf+'_'+str(args.epochs)\n",
    "# print(vars(args))\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = args.device_id\n",
    "\n",
    "# # make output directory if it doesn't already exist\n",
    "# if os.path.isdir(args.outf):\n",
    "#     shutil.rmtree(args.outf)\n",
    "# os.makedirs(args.outf)\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "# random.seed(args.seed)\n",
    "# np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     if not args.cuda:\n",
    "#         print(\"WARNING: You have a CUDA device, \"\n",
    "#               \"so you should probably run with --cuda\")\n",
    "#     else:\n",
    "#         torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab 47917; Pruned to 30004\n",
      "Number of sentences dropped from data_fr/valid1.txt: 1 out of 1019 total\n",
      "Number of sentences dropped from data_fr/valid2.txt: 0 out of 1332 total\n",
      "Number of sentences dropped from data_fr/train1.txt: 237 out of 51967 total\n",
      "Number of sentences dropped from data_fr/train2.txt: 33 out of 51967 total\n",
      "808 batches\n",
      "data_path is data_fr\n",
      "source shape is torch.Size([64, 23])\n",
      "lengths shape is 64\n",
      "code shape is torch.Size([64, 256])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7ed22af6c345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code shape is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded data!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "label_ids = {\"pos\": 1, \"neg\": 0}\n",
    "id2label = {1: \"pos\", 0: \"neg\"}\n",
    "\n",
    "# (Path to textfile, Name, Use4Vocab)\n",
    "datafiles = [(os.path.join(args.data_path, \"valid1.txt\"), \"valid1\", False),\n",
    "             (os.path.join(args.data_path, \"valid2.txt\"), \"valid2\", False),\n",
    "             (os.path.join(args.data_path, \"train1.txt\"), \"train1\", True),\n",
    "             (os.path.join(args.data_path, \"train2.txt\"), \"train2\", True)]\n",
    "vocabdict = None\n",
    "if args.load_vocab != \"\":\n",
    "    vocabdict = json.load(args.vocab)\n",
    "    vocabdict = {k: int(v) for k, v in vocabdict.items()}\n",
    "corpus = Corpus(datafiles,\n",
    "                maxlen=args.maxlen,\n",
    "                vocab_size=args.vocab_size,\n",
    "                lowercase=args.lowercase,\n",
    "                vocab=vocabdict)\n",
    "\n",
    "# # dumping vocabulary\n",
    "# with open('{}/vocab.json'.format(args.outf), 'w') as f:\n",
    "#     json.dump(corpus.dictionary.word2idx, f)\n",
    "\n",
    "# # save arguments\n",
    "# ntokens = len(corpus.dictionary.word2idx)\n",
    "# print(\"Vocabulary Size: {}\".format(ntokens))\n",
    "# args.ntokens = ntokens\n",
    "# with open('{}/args.json'.format(args.outf), 'w') as f:\n",
    "#     json.dump(vars(args), f)\n",
    "# with open(\"{}/log.txt\".format(args.outf), 'w') as f:\n",
    "#     f.write(str(vars(args)))\n",
    "#     f.write(\"\\n\\n\")\n",
    "\n",
    "eval_batch_size = 100\n",
    "# test1_data = batchify(corpus.data['valid1'], eval_batch_size, shuffle=False)\n",
    "# test2_data = batchify(corpus.data['valid2'], eval_batch_size, shuffle=False)\n",
    "train1_data = batchify(corpus.data['train1'], args.batch_size, shuffle=True)\n",
    "\n",
    "print(\"data_path is {}\".format(args.data_path))\n",
    "# get input shape of autoencoder\n",
    "source, target, lengths = train1_data[0]\n",
    "print(\"source shape is {}\".format(source.shape))#(64,25)\n",
    "print(\"lengths shape is {}\".format(len(lengths))) # (64,25)\n",
    "# train2_data = batchify(corpus.data['train2'], args.batch_size, shuffle=True)\n",
    "# output of autoencoder is input of discriminator\n",
    "\n",
    "\n",
    "autoencoder = Seq2Seq2Decoder(emsize=256,\n",
    "                              nhidden=256,                              \n",
    "                              ntokens=30004,\n",
    "                              nlayers=1,\n",
    "                              noise_r=0.1,\n",
    "                              hidden_init=False,\n",
    "                              dropout=0.0,\n",
    "                              gpu=None)\n",
    "\n",
    "\n",
    "code=autoencoder(0,torch.ones(64,25,dtype=torch.long),noise=False, encode_only=True)\n",
    "print(\"code shape is {}\".format(code.shape))\n",
    "\n",
    "assert(1 == 2)\n",
    "\n",
    "print(\"Loaded data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Build model#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component to experiment with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models1 import Seq2Seq2Decoder, Seq2Seq, MLP_G, MLP_Classify, CNN_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "autoencoder = Seq2Seq2Decoder(emsize=args.emsize,\\\n",
    "                              nhidden=args.nhidden,\\\n",
    "                              ntokens=ntokens,\\\n",
    "                              nlayers=args.nlayers,\\\n",
    "                              noise_r=args.noise_r,\\\n",
    "                              hidden_init=args.hidden_init,\\\n",
    "                              dropout=args.dropout,\\\n",
    "                              gpu=args.cuda)\n",
    "\n",
    "print(args.arch_g)\n",
    "print(args.arch_d)\n",
    "gan_gen = MLP_G(ninput=args.z_size, noutput=args.nhidden, layers=args.arch_g)\n",
    "#TODO\n",
    "print(args)\n",
    "gan_disc = CNN_D(ninput=args.batch_size, noutput=args.nhidden, filter_sizes='1-3-5')#, layers=args.arch_d)\n",
    "print(gan_disc)\n",
    "#pdb.set_trace()\n",
    "classifier = MLP_Classify(\n",
    "    ninput=args.nhidden, noutput=1, layers=args.arch_classify)\n",
    "g_factor = None\n",
    "\n",
    "print(autoencoder)\n",
    "print(gan_gen)\n",
    "print(gan_disc)\n",
    "print(classifier)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=args.lr_ae)\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\\\n",
    "                             lr=args.lr_gan_g,\\\n",
    "                             betas=(args.beta1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\\\n",
    "                             lr=args.lr_gan_d,\\\n",
    "                             betas=(args.beta1, 0.999))\n",
    "# classify\n",
    "optimizer_classify = optim.Adam(classifier.parameters(),\\\n",
    "                                lr=args.lr_classify,\\\n",
    "                                betas=(args.beta1, 0.999))\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "if args.cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    gan_gen = gan_gen.cuda()\n",
    "    gan_disc = gan_disc.cuda()\n",
    "    classifier = classifier.cuda()\n",
    "    criterion_ce = criterion_ce.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    print(\"Saving models\")\n",
    "    with open('{}/autoencoder_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(autoencoder.state_dict(), f)\n",
    "    with open('{}/gan_gen_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(gan_gen.state_dict(), f)\n",
    "    with open('{}/gan_disc_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(gan_disc.state_dict(), f)\n",
    "\n",
    "\n",
    "def train_classifier(whichclass, batch):\n",
    "    classifier.train()\n",
    "    classifier.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    labels = to_gpu(args.cuda, Variable(\n",
    "        torch.zeros(source.size(0)).fill_(whichclass - 1)))\n",
    "\n",
    "    # Train\n",
    "    code = autoencoder(0, source, lengths, noise=False,\n",
    "                       encode_only=True).detach()\n",
    "    scores = classifier(code)\n",
    "    classify_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "    classify_loss.backward()\n",
    "    optimizer_classify.step()\n",
    "    classify_loss = classify_loss.cpu().data[0]\n",
    "\n",
    "    pred = scores.data.round().squeeze(1)\n",
    "    accuracy = pred.eq(labels.data).float().mean()\n",
    "\n",
    "    return classify_loss, accuracy\n",
    "\n",
    "\n",
    "def grad_hook_cla(grad):\n",
    "    return grad * args.lambda_class\n",
    "\n",
    "\n",
    "def classifier_regularize(whichclass, batch):\n",
    "    autoencoder.train()\n",
    "    autoencoder.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "    flippedclass = abs(2 - whichclass)\n",
    "    labels = to_gpu(args.cuda, Variable(\n",
    "        torch.zeros(source.size(0)).fill_(flippedclass)))\n",
    "\n",
    "    # Train\n",
    "    code = autoencoder(0, source, lengths, noise=False, encode_only=True)\n",
    "    code.register_hook(grad_hook_cla)\n",
    "    scores = classifier(code)\n",
    "    classify_reg_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "    classify_reg_loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), args.clip)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    return classify_reg_loss\n",
    "\n",
    "\n",
    "def evaluate_autoencoder(whichdecoder, data_source, epoch):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary.word2idx)\n",
    "    all_accuracies = 0\n",
    "    bcnt = 0\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        source = to_gpu(args.cuda, Variable(source, volatile=True))\n",
    "        target = to_gpu(args.cuda, Variable(target, volatile=True))\n",
    "\n",
    "        mask = target.gt(0)\n",
    "        masked_target = target.masked_select(mask)\n",
    "        # examples x ntokens\n",
    "        output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "\n",
    "        hidden = autoencoder(0, source, lengths, noise=False, encode_only=True)\n",
    "\n",
    "        # output: batch x seq_len x ntokens\n",
    "        if whichdecoder == 1:\n",
    "            output = autoencoder(1, source, lengths, noise=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            # accuracy\n",
    "            max_vals1, max_indices1 = torch.max(masked_output, 1)\n",
    "            all_accuracies += \\\n",
    "                torch.mean(max_indices1.eq(masked_target).float()).data[0]\n",
    "\n",
    "            max_values1, max_indices1 = torch.max(output, 2)\n",
    "            max_indices2 = autoencoder.generate(2, hidden, maxlen=50)\n",
    "        else:\n",
    "            output = autoencoder(2, source, lengths, noise=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            # accuracy\n",
    "            max_vals2, max_indices2 = torch.max(masked_output, 1)\n",
    "            all_accuracies += \\\n",
    "                torch.mean(max_indices2.eq(masked_target).float()).data[0]\n",
    "\n",
    "            max_values2, max_indices2 = torch.max(output, 2)\n",
    "            max_indices1 = autoencoder.generate(1, hidden, maxlen=50)\n",
    "\n",
    "        total_loss += criterion_ce(masked_output /\n",
    "                                   args.temp, masked_target).data\n",
    "        bcnt += 1\n",
    "\n",
    "        aeoutf_from = \"{}/{}_output_decoder_{}_from.txt\".format(\n",
    "            args.outf, epoch, whichdecoder)\n",
    "        aeoutf_tran = \"{}/{}_output_decoder_{}_tran.txt\".format(\n",
    "            args.outf, epoch, whichdecoder)\n",
    "        with open(aeoutf_from, 'w') as f_from, open(aeoutf_tran, 'w') as f_trans:\n",
    "            max_indices1 = \\\n",
    "                max_indices1.view(output.size(0), -1).data.cpu().numpy()\n",
    "            max_indices2 = \\\n",
    "                max_indices2.view(output.size(0), -1).data.cpu().numpy()\n",
    "            target = target.view(output.size(0), -1).data.cpu().numpy()\n",
    "            tran_indices = max_indices2 if whichdecoder == 1 else max_indices1\n",
    "            for t, tran_idx in zip(target, tran_indices):\n",
    "                # real sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in t])\n",
    "                f_from.write(chars)\n",
    "                f_from.write(\"\\n\")\n",
    "                # transfer sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x]\n",
    "                                  for x in tran_idx])\n",
    "                f_trans.write(chars)\n",
    "                f_trans.write(\"\\n\")\n",
    "\n",
    "    return total_loss[0] / len(data_source), all_accuracies / bcnt\n",
    "\n",
    "\n",
    "def evaluate_generator(whichdecoder, noise, epoch):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # generate from fixed random noise\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = \\\n",
    "        autoencoder.generate(whichdecoder, fake_hidden,\n",
    "                             maxlen=50, sample=args.sample)\n",
    "\n",
    "    with open(\"%s/%s_generated%d.txt\" % (args.outf, epoch, whichdecoder), \"w\") as f:\n",
    "        max_indices = max_indices.data.cpu().numpy()\n",
    "        for idx in max_indices:\n",
    "            # generated sentence\n",
    "            words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "            # truncate sentences to first occurrence of <eos>\n",
    "            truncated_sent = []\n",
    "            for w in words:\n",
    "                if w != '<eos>':\n",
    "                    truncated_sent.append(w)\n",
    "                else:\n",
    "                    break\n",
    "            chars = \" \".join(truncated_sent)\n",
    "            f.write(chars)\n",
    "            print(\"word length is:\", len(words))\n",
    "            print('generated output:\\n', chars)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def train_ae(whichdecoder, batch, total_loss_ae, start_time, i):\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "\n",
    "    mask = target.gt(0)\n",
    "    masked_target = target.masked_select(mask)\n",
    "    output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "    output = autoencoder(whichdecoder, source, lengths, noise=True)\n",
    "    flat_output = output.view(-1, ntokens)\n",
    "    masked_output = flat_output.masked_select(output_mask).view(-1, ntokens)\n",
    "    loss = criterion_ce(masked_output / args.temp, masked_target)\n",
    "    loss.backward()\n",
    "\n",
    "    # `clip_grad_norm` to prevent exploding gradient in RNNs / LSTMs\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), args.clip)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    total_loss_ae += loss.data\n",
    "\n",
    "    accuracy = None\n",
    "    if i % args.log_interval == 0 and i > 0:\n",
    "        probs = F.softmax(masked_output, dim=-1)\n",
    "        max_vals, max_indices = torch.max(probs, 1)\n",
    "        accuracy = torch.mean(max_indices.eq(masked_target).float()).data[0]\n",
    "        cur_loss = total_loss_ae[0] / args.log_interval\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "              'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}'\n",
    "              .format(epoch, i, len(train1_data),\n",
    "                      elapsed * 1000 / args.log_interval,\n",
    "                      cur_loss, math.exp(cur_loss), accuracy))\n",
    "\n",
    "        with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "            f.write('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}\\n'.\n",
    "                    format(epoch, i, len(train1_data),\n",
    "                           elapsed * 1000 / args.log_interval,\n",
    "                           cur_loss, math.exp(cur_loss), accuracy))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    return total_loss_ae, start_time\n",
    "\n",
    "\n",
    "def train_gan_g():\n",
    "    gan_gen.train()\n",
    "    gan_gen.zero_grad()\n",
    "\n",
    "    noise = to_gpu(args.cuda,Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "    noise.data.normal_(0, 1)\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    errG = gan_disc(fake_hidden)\n",
    "#     print(\"shape of errG is: {}\".format(errG.shape))\n",
    "    errG.backward(one)\n",
    "    optimizer_gan_g.step()\n",
    "\n",
    "    return errG\n",
    "\n",
    "\n",
    "def grad_hook(grad):\n",
    "    return grad * args.grad_lambda\n",
    "\n",
    "\n",
    "''' Steal from https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py '''\n",
    "\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    bsz = real_data.size(0)\n",
    "    alpha = torch.rand(bsz, 1)\n",
    "    alpha = alpha.expand(bsz, real_data.size(1))  # only works for 2D XXX\n",
    "    alpha = alpha.cuda()\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\\\n",
    "                                    grad_outputs=torch.ones(\\\n",
    "                                        disc_interpolates.size()).cuda(),\\\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1)\n",
    "                        ** 2).mean() * args.gan_gp_lambda\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def train_gan_d(whichdecoder, batch):\n",
    "    gan_disc.train()\n",
    "    optimizer_gan_d.zero_grad()\n",
    "\n",
    "    # positive samples ----------------------------\n",
    "    # generate real codes\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "\n",
    "    # batch_size x nhidden\n",
    "    real_hidden = autoencoder(whichdecoder, source,lengths, noise=False, encode_only=True)\n",
    "\n",
    "    # loss / backprop\n",
    "    errD_real = gan_disc(real_hidden)\n",
    "    errD_real.backward(one)\n",
    "\n",
    "    # negative samples ----------------------------\n",
    "    # generate fake codes\n",
    "    noise = to_gpu(args.cuda,\n",
    "                Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "    noise.data.normal_(0, 1)\n",
    "\n",
    "    # loss / backprop\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    errD_fake = gan_disc(fake_hidden.detach())\n",
    "    errD_fake.backward(mone)\n",
    "\n",
    "    # gradient penalty\n",
    "    gradient_penalty = calc_gradient_penalty(gan_disc, real_hidden.data, fake_hidden.data)\n",
    "    gradient_penalty.backward()\n",
    "\n",
    "    optimizer_gan_d.step()\n",
    "    errD = -(errD_real - errD_fake)\n",
    "\n",
    "    return errD, errD_real, errD_fake\n",
    "\n",
    "\n",
    "def train_gan_d_into_ae(whichdecoder, batch):\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "    real_hidden = autoencoder(whichdecoder, source,lengths, noise=False, encode_only=True)\n",
    "    real_hidden.register_hook(grad_hook)\n",
    "    errD_real = gan_disc(real_hidden)\n",
    "    errD_real.backward(mone)\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), args.clip)\n",
    "\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    return errD_real"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training...\")\n",
    "with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "    f.write('Training...\\n')\n",
    "\n",
    "# schedule of increasing GAN training loops\n",
    "if args.niters_gan_schedule != \"\":\n",
    "    gan_schedule = [int(x) for x in args.niters_gan_schedule.split(\"-\")]\n",
    "else:\n",
    "    gan_schedule = []\n",
    "niter_gan = 1\n",
    "\n",
    "fixed_noise = to_gpu(args.cuda,Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "fixed_noise.data.normal_(0, 1)\n",
    "one = to_gpu(args.cuda, torch.FloatTensor([1]))\n",
    "mone = one * -1\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    # update gan training schedule\n",
    "    if epoch in gan_schedule:\n",
    "        niter_gan += 1\n",
    "        print(\"GAN training loop schedule increased to {}\".format(niter_gan))\n",
    "        with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "            f.write(\"GAN training loop schedule increased to {}\\n\".\n",
    "                    format(niter_gan))\n",
    "    if epoch > 0 and epoch%10 == 0:\n",
    "        args.lr_gan_d=args.lr_gan_d/2\n",
    "            \n",
    "    total_loss_ae1 = 0\n",
    "    total_loss_ae2 = 0\n",
    "    classify_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "    start_time = time.time()\n",
    "    niter = 0\n",
    "    niter_global = 1\n",
    "\n",
    "    # loop through all batches in training data\n",
    "    while niter < len(train1_data) and niter < len(train2_data):\n",
    "\n",
    "        # train autoencoder ----------------------------\n",
    "        for i in range(args.niters_ae):\n",
    "            if niter == len(train1_data):\n",
    "                break  # end of epoch\n",
    "            total_loss_ae1, start_time = train_ae(1, train1_data[niter], total_loss_ae1, start_time, niter)\n",
    "            total_loss_ae2, _ = train_ae(2, train2_data[niter], total_loss_ae2, start_time, niter)\n",
    "\n",
    "            # train classifier ----------------------------\n",
    "            classify_loss1, classify_acc1 = train_classifier(\n",
    "                1, train1_data[niter])\n",
    "            classify_loss2, classify_acc2 = train_classifier(\n",
    "                2, train2_data[niter])\n",
    "            classify_loss = (classify_loss1 + classify_loss2) / 2\n",
    "            classify_acc = (classify_acc1 + classify_acc2) / 2\n",
    "            # reverse to autoencoder\n",
    "            classifier_regularize(1, train1_data[niter])\n",
    "            classifier_regularize(2, train2_data[niter])\n",
    "\n",
    "            niter += 1\n",
    "\n",
    "        # train gan ----------------------------------\n",
    "        for k in range(niter_gan):\n",
    "\n",
    "            # train discriminator/critic\n",
    "            for i in range(args.niters_gan_d):\n",
    "                # feed a seen sample within this epoch; good for early training\n",
    "                if i % 2 == 0:\n",
    "                    batch = train1_data[\n",
    "                        random.randint(0, len(train1_data) - 1)]\n",
    "                    whichdecoder = 1\n",
    "                else:\n",
    "                    batch = train2_data[\n",
    "                        random.randint(0, len(train2_data) - 1)]\n",
    "                    whichdecoder = 2\n",
    "                errD, errD_real, errD_fake = train_gan_d(whichdecoder, batch)\n",
    "\n",
    "            # train generator\n",
    "            for i in range(args.niters_gan_g):\n",
    "                errG = train_gan_g()\n",
    "\n",
    "            # train autoencoder from d\n",
    "            for i in range(args.niters_gan_ae):\n",
    "                if i % 2 == 0:\n",
    "                    batch = train1_data[\n",
    "                        random.randint(0, len(train1_data) - 1)]\n",
    "                    whichdecoder = 1\n",
    "                else:\n",
    "                    batch = train2_data[\n",
    "                        random.randint(0, len(train2_data) - 1)]\n",
    "                    whichdecoder = 2\n",
    "                errD_ = train_gan_d_into_ae(whichdecoder, batch)\n",
    "\n",
    "        niter_global += 1\n",
    "        if niter_global % 100 == 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f (Loss_D_real: %.4f '\n",
    "                  'Loss_D_fake: %.4f) Loss_G: %.4f'\n",
    "                  % (epoch, args.epochs, niter, len(train1_data),\n",
    "                     errD.data[0], errD_real.data[0],\n",
    "                     errD_fake.data[0], errG.data[0]))\n",
    "            print(\"Classify loss: {:5.2f} | Classify accuracy: {:3.3f}\\n\".format(\n",
    "                classify_loss, classify_acc))\n",
    "            with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "                f.write('[%d/%d][%d/%d] Loss_D: %.4f (Loss_D_real: %.4f '\n",
    "                        'Loss_D_fake: %.4f) Loss_G: %.4f\\n'\n",
    "                        % (epoch, args.epochs, niter, len(train1_data),\n",
    "                           errD.data[0], errD_real.data[0],\n",
    "                           errD_fake.data[0], errG.data[0]))\n",
    "                f.write(\"Classify loss: {:5.2f} | Classify accuracy: {:3.3f}\\n\".format(\n",
    "                        classify_loss, classify_acc))\n",
    "\n",
    "            # exponentially decaying noise on autoencoder\n",
    "            autoencoder.noise_r = \\\n",
    "                autoencoder.noise_r * args.noise_anneal\n",
    "\n",
    "    # end of epoch ----------------------------\n",
    "    # evaluation\n",
    "    test_loss, accuracy = evaluate_autoencoder(1, test1_data[:1000], epoch)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "          'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "          format(epoch, (time.time() - epoch_start_time),\n",
    "                 test_loss, math.exp(test_loss), accuracy))\n",
    "    print('-' * 89)\n",
    "    with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "                ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "                format(epoch, (time.time() - epoch_start_time),\n",
    "                       test_loss, math.exp(test_loss), accuracy))\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n')\n",
    "\n",
    "    test_loss, accuracy = evaluate_autoencoder(2, test2_data[:1000], epoch)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "          'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "          format(epoch, (time.time() - epoch_start_time),\n",
    "                 test_loss, math.exp(test_loss), accuracy))\n",
    "    print('-' * 89)\n",
    "    with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "                ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "                format(epoch, (time.time() - epoch_start_time),\n",
    "                       test_loss, math.exp(test_loss), accuracy))\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n')\n",
    "\n",
    "    evaluate_generator(1, fixed_noise, \"end_of_epoch_{}\".format(epoch))\n",
    "    evaluate_generator(2, fixed_noise, \"end_of_epoch_{}\".format(epoch))\n",
    "\n",
    "    # shuffle between epochs\n",
    "    train1_data = batchify(\n",
    "        corpus.data['train1'], args.batch_size, shuffle=True)\n",
    "    train2_data = batchify(\n",
    "        corpus.data['train2'], args.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_loss, accuracy = evaluate_autoencoder(1, test1_data, epoch + 1)\n",
    "print('-' * 89)\n",
    "print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "      'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "      format(epoch, (time.time() - epoch_start_time),\n",
    "             test_loss, math.exp(test_loss), accuracy))\n",
    "print('-' * 89)\n",
    "with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "            ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "            format(epoch, (time.time() - epoch_start_time),\n",
    "                   test_loss, math.exp(test_loss), accuracy))\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n')\n",
    "\n",
    "test_loss, accuracy = evaluate_autoencoder(2, test2_data, epoch + 1)\n",
    "print('-' * 89)\n",
    "print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "      'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "      format(epoch, (time.time() - epoch_start_time),\n",
    "             test_loss, math.exp(test_loss), accuracy))\n",
    "print('-' * 89)\n",
    "with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "            ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "            format(epoch, (time.time() - epoch_start_time),\n",
    "                   test_loss, math.exp(test_loss), accuracy))\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
