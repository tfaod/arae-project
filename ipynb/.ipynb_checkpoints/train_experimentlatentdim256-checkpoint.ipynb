{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, Corpus, batchify\n",
    "from models1 import Seq2Seq2Decoder, Seq2Seq, MLP_G, MLP_Classify\n",
    "from models import MLP_D\n",
    "import shutil\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': 'data_yelp', 'outf': 'data_yelp_output_50d', 'load_vocab': '', 'vocab_size': 30000, 'maxlen': 25, 'lowercase': True, 'emsize': 256, 'nhidden': 256, 'nlayers': 1, 'noise_r': 0.1, 'noise_anneal': 0.9995, 'hidden_init': False, 'arch_g': '256-256', 'arch_d': '256-256', 'arch_classify': '256-256', 'z_size': 64, 'temp': 1, 'dropout': 0.0, 'epochs': 50, 'batch_size': 64, 'niters_ae': 10, 'niters_gan_d': 10, 'niters_gan_g': 5, 'niters_gan_ae': 5, 'niters_gan_schedule': '', 'lr_ae': 1, 'lr_gan_g': 0.001, 'lr_gan_d': 0.001, 'lr_classify': 0.001, 'beta1': 0.5, 'clip': 1, 'gan_gp_lambda': 0.1, 'grad_lambda': 0.01, 'lambda_class': 1, 'sample': False, 'log_interval': 200, 'seed': 1111, 'cuda': True, 'device_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='ARAE for Yelp transfer')\n",
    "# Path Arguments\n",
    "parser.add_argument('--data_path', type=str, required=True,\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--outf', type=str, default=\"_output\",\n",
    "                    help='output directory name')\n",
    "parser.add_argument('--load_vocab', type=str, default=\"\",\n",
    "                    help='path to load vocabulary from')\n",
    "\n",
    "# Data Processing Arguments\n",
    "parser.add_argument('--vocab_size', type=int, default=30000,\n",
    "                    help='cut vocabulary down to this size '\n",
    "                         '(most frequently seen words in train)')\n",
    "parser.add_argument('--maxlen', type=int, default=25,\n",
    "                    help='maximum sentence length')\n",
    "parser.add_argument('--lowercase', dest='lowercase', action='store_true',\n",
    "                    help='lowercase all text')\n",
    "parser.add_argument('--no-lowercase', dest='lowercase', action='store_true',\n",
    "                    help='not lowercase all text')\n",
    "parser.set_defaults(lowercase=True)\n",
    "\n",
    "# Model Argument\n",
    "parser.add_argument('--emsize', type=int, default=256,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhidden', type=int, default=256,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=1,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--noise_r', type=float, default=0.1,\n",
    "                    help='stdev of noise for autoencoder (regularizer)')\n",
    "parser.add_argument('--noise_anneal', type=float, default=0.9995,\n",
    "                    help='anneal noise_r exponentially by this'\n",
    "                         'every 100 iterations')\n",
    "parser.add_argument('--hidden_init', action='store_true',\n",
    "                    help=\"initialize decoder hidden state with encoder's\")\n",
    "parser.add_argument('--arch_g', type=str, default='256-256',\n",
    "                    help='generator architecture (MLP)')\n",
    "parser.add_argument('--arch_d', type=str, default='256-256',\n",
    "                    help='critic/discriminator architecture (MLP)')\n",
    "parser.add_argument('--arch_classify', type=str, default='256-256',\n",
    "                    help='classifier architecture')\n",
    "parser.add_argument('--z_size', type=int, default=64,\n",
    "                    help='dimension of random noise z to feed into generator')\n",
    "parser.add_argument('--temp', type=float, default=1,\n",
    "                    help='softmax temperature (lower --> more discrete)')\n",
    "parser.add_argument('--dropout', type=float, default=0.0,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "\n",
    "# Training Arguments\n",
    "epoch_num = 50\n",
    "parser.add_argument('--epochs', type=int, default=epoch_num, #changed from 25\n",
    "                    help='maximum number of epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--niters_ae', type=int, default=10,\n",
    "                    help='number of autoencoder iterations in training')\n",
    "parser.add_argument('--niters_gan_d', type=int, default=10,\n",
    "                    help='number of discriminator iterations in training')\n",
    "parser.add_argument('--niters_gan_g', type=int, default=5,\n",
    "                    help='number of generator iterations in training')\n",
    "parser.add_argument('--niters_gan_ae', type=int, default=5,\n",
    "                    help='number of gan-into-ae iterations in training')\n",
    "parser.add_argument('--niters_gan_schedule', type=str, default='',\n",
    "                    help='epoch counts to increase number of GAN training '\n",
    "                         ' iterations (increment by 1 each time)')\n",
    "parser.add_argument('--lr_ae', type=float, default=1,\n",
    "                    help='autoencoder learning rate')\n",
    "parser.add_argument('--lr_gan_g', type=float, default=1e-03,\n",
    "                    help='generator learning rate')\n",
    "parser.add_argument('--lr_gan_d', type=float, default=1e-03,\n",
    "                    help='critic/discriminator learning rate')\n",
    "parser.add_argument('--lr_classify', type=float, default=1e-03,\n",
    "                    help='classifier learning rate')\n",
    "parser.add_argument('--beta1', type=float, default=0.5,\n",
    "                    help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--clip', type=float, default=1,\n",
    "                    help='gradient clipping, max norm')\n",
    "parser.add_argument('--gan_gp_lambda', type=float, default=0.1,\n",
    "                    help='WGAN GP penalty lambda')\n",
    "parser.add_argument('--grad_lambda', type=float, default=0.01,\n",
    "                    help='WGAN into AE lambda')\n",
    "parser.add_argument('--lambda_class', type=float, default=1,\n",
    "                    help='lambda on classifier')\n",
    "\n",
    "# Evaluation Arguments\n",
    "parser.add_argument('--sample', action='store_true',\n",
    "                    help='sample when decoding for generation')\n",
    "parser.add_argument('--log_interval', type=int, default=200,\n",
    "                    help='interval to log autoencoder training results')\n",
    "\n",
    "# Other\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', dest='cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--no-cuda', dest='cuda', action='store_true',\n",
    "                    help='not using CUDA')\n",
    "parser.set_defaults(cuda=True)\n",
    "parser.add_argument('--device_id', type=str, default='0')\n",
    "args = parser.parse_args(['--data_path', 'data_yelp'])\n",
    "if(args.outf == \"_output\"):\n",
    "    args.outf = args.data_path + args.outf+'_'+str(args.epochs)+'d'\n",
    "print(vars(args))\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_id\n",
    "\n",
    "# make output directory if it doesn't already exist\n",
    "if os.path.isdir(args.outf):\n",
    "    shutil.rmtree(args.outf)\n",
    "os.makedirs(args.outf)\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, \"\n",
    "              \"so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='ARAE for Yelp transfer')\n",
    "# Path Arguments\n",
    "parser.add_argument('--data_path', type=str, required=True,\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--outf', type=str, default=\"_output\",\n",
    "                    help='output directory name')\n",
    "parser.add_argument('--load_vocab', type=str, default=\"\",\n",
    "                    help='path to load vocabulary from')\n",
    "\n",
    "# Data Processing Arguments\n",
    "parser.add_argument('--vocab_size', type=int, default=30000,\n",
    "                    help='cut vocabulary down to this size '\n",
    "                         '(most frequently seen words in train)')\n",
    "parser.add_argument('--maxlen', type=int, default=25,\n",
    "                    help='maximum sentence length')\n",
    "parser.add_argument('--lowercase', dest='lowercase', action='store_true',\n",
    "                    help='lowercase all text')\n",
    "parser.add_argument('--no-lowercase', dest='lowercase', action='store_true',\n",
    "                    help='not lowercase all text')\n",
    "parser.set_defaults(lowercase=True)\n",
    "\n",
    "# Model Arguments\n",
    "parser.add_argument('--emsize', type=int, default=128,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhidden', type=int, default=128,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=1,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--noise_r', type=float, default=0.1,\n",
    "                    help='stdev of noise for autoencoder (regularizer)')\n",
    "parser.add_argument('--noise_anneal', type=float, default=0.9995,\n",
    "                    help='anneal noise_r exponentially by this'\n",
    "                         'every 100 iterations')\n",
    "parser.add_argument('--hidden_init', action='store_true',\n",
    "                    help=\"initialize decoder hidden state with encoder's\")\n",
    "parser.add_argument('--arch_g', type=str, default='128-128',\n",
    "                    help='generator architecture (MLP)')\n",
    "parser.add_argument('--arch_d', type=str, default='128-128',\n",
    "                    help='critic/discriminator architecture (MLP)')\n",
    "parser.add_argument('--arch_classify', type=str, default='128-128',\n",
    "                    help='classifier architecture')\n",
    "parser.add_argument('--z_size', type=int, default=32,\n",
    "                    help='dimension of random noise z to feed into generator')\n",
    "parser.add_argument('--temp', type=float, default=1,\n",
    "                    help='softmax temperature (lower --> more discrete)')\n",
    "parser.add_argument('--dropout', type=float, default=0.0,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "\n",
    "# Training Arguments\n",
    "parser.add_argument('--epochs', type=int, default=25,\n",
    "                    help='maximum number of epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--niters_ae', type=int, default=1,\n",
    "                    help='number of autoencoder iterations in training')\n",
    "parser.add_argument('--niters_gan_d', type=int, default=5,\n",
    "                    help='number of discriminator iterations in training')\n",
    "parser.add_argument('--niters_gan_g', type=int, default=1,\n",
    "                    help='number of generator iterations in training')\n",
    "parser.add_argument('--niters_gan_ae', type=int, default=1,\n",
    "                    help='number of gan-into-ae iterations in training')\n",
    "parser.add_argument('--niters_gan_schedule', type=str, default='',\n",
    "                    help='epoch counts to increase number of GAN training '\n",
    "                         ' iterations (increment by 1 each time)')\n",
    "parser.add_argument('--lr_ae', type=float, default=1,\n",
    "                    help='autoencoder learning rate')\n",
    "parser.add_argument('--lr_gan_g', type=float, default=1e-04,\n",
    "                    help='generator learning rate')\n",
    "parser.add_argument('--lr_gan_d', type=float, default=1e-04,\n",
    "                    help='critic/discriminator learning rate')\n",
    "parser.add_argument('--lr_classify', type=float, default=1e-04,\n",
    "                    help='classifier learning rate')\n",
    "parser.add_argument('--beta1', type=float, default=0.5,\n",
    "                    help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--clip', type=float, default=1,\n",
    "                    help='gradient clipping, max norm')\n",
    "parser.add_argument('--gan_gp_lambda', type=float, default=0.1,\n",
    "                    help='WGAN GP penalty lambda')\n",
    "parser.add_argument('--grad_lambda', type=float, default=0.01,\n",
    "                    help='WGAN into AE lambda')\n",
    "parser.add_argument('--lambda_class', type=float, default=1,\n",
    "                    help='lambda on classifier')\n",
    "\n",
    "# Evaluation Arguments\n",
    "parser.add_argument('--sample', action='store_true',\n",
    "                    help='sample when decoding for generation')\n",
    "parser.add_argument('--log_interval', type=int, default=1,\n",
    "                    help='interval to log autoencoder training results')\n",
    "\n",
    "# Other\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', dest='cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--no-cuda', dest='cuda', action='store_true',\n",
    "                    help='not using CUDA')\n",
    "parser.set_defaults(cuda=True)\n",
    "parser.add_argument('--device_id', type=str, default='0')\n",
    "\n",
    "args = parser.parse_args(['--data_path', 'data_yelp'])\n",
    "#args = parser.parse_args()\n",
    "if(args.outf==\"_output\"):\n",
    "    args.outf = args.data_path + args.outf\n",
    "print(vars(args))\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_id\n",
    "\n",
    "# make output directory if it doesn't already exist\n",
    "if os.path.isdir(args.outf):\n",
    "    shutil.rmtree(args.outf)\n",
    "os.makedirs(args.outf)\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, \"\n",
    "              \"so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab 9599; Pruned to 9603\n",
      "Number of sentences dropped from data_yelp/valid1.txt: 0 out of 38205 total\n",
      "Number of sentences dropped from data_yelp/valid2.txt: 0 out of 25278 total\n",
      "Number of sentences dropped from data_yelp/train1.txt: 0 out of 267314 total\n",
      "Number of sentences dropped from data_yelp/train2.txt: 0 out of 176787 total\n",
      "Vocabulary Size: 9603\n",
      "382 batches\n",
      "252 batches\n",
      "4176 batches\n",
      "2762 batches\n",
      "Loaded data!\n"
     ]
    }
   ],
   "source": [
    "label_ids = {\"pos\": 1, \"neg\": 0}\n",
    "id2label = {1: \"pos\", 0: \"neg\"}\n",
    "\n",
    "# (Path to textfile, Name, Use4Vocab)\n",
    "datafiles = [(os.path.join(args.data_path, \"valid1.txt\"), \"valid1\", False),\n",
    "             (os.path.join(args.data_path, \"valid2.txt\"), \"valid2\", False),\n",
    "             (os.path.join(args.data_path, \"train1.txt\"), \"train1\", True),\n",
    "             (os.path.join(args.data_path, \"train2.txt\"), \"train2\", True)]\n",
    "vocabdict = None\n",
    "if args.load_vocab != \"\":\n",
    "    vocabdict = json.load(args.vocab)\n",
    "    vocabdict = {k: int(v) for k, v in vocabdict.items()}\n",
    "corpus = Corpus(datafiles,\n",
    "                maxlen=args.maxlen,\n",
    "                vocab_size=args.vocab_size,\n",
    "                lowercase=args.lowercase,\n",
    "                vocab=vocabdict)\n",
    "\n",
    "# dumping vocabulary\n",
    "with open('{}/vocab.json'.format(args.outf), 'w') as f:\n",
    "    json.dump(corpus.dictionary.word2idx, f)\n",
    "\n",
    "# save arguments\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(ntokens))\n",
    "args.ntokens = ntokens\n",
    "with open('{}/args.json'.format(args.outf), 'w') as f:\n",
    "    json.dump(vars(args), f)\n",
    "with open(\"{}/log.txt\".format(args.outf), 'w') as f:\n",
    "    f.write(str(vars(args)))\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "eval_batch_size = 100\n",
    "test1_data = batchify(corpus.data['valid1'], eval_batch_size, shuffle=False)\n",
    "test2_data = batchify(corpus.data['valid2'], eval_batch_size, shuffle=False)\n",
    "train1_data = batchify(corpus.data['train1'], args.batch_size, shuffle=True)\n",
    "train2_data = batchify(corpus.data['train2'], args.batch_size, shuffle=True)\n",
    "\n",
    "print(\"Loaded data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Build model#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component to experiment with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ninput, noutput, layers, activation=nn.LeakyReLU(0.2), gpu=False):\n",
    "        #pdb.set_trace()\n",
    "        super(CNN_D, self).__init__()\n",
    "        self.ninput = ninput\n",
    "        self.noutput = noutput\n",
    "        self.conv1 = nn.Conv2d(self.ninput, 1, kernel_size=4,\\\n",
    "                               stride=1, padding=2, bias=True)\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        #pdb.set_trace()\n",
    "        # flatten before feeding into fully connected\n",
    "        self.fc1 = nn.Linear(256, noutput, bias=True)  # calculate size here,1 bias = True)\n",
    "        #256 is palce holder\n",
    "    def forward(self, x):\n",
    "        #pdb.set_trace()\n",
    "        # get last item\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        #print(x.shape)\n",
    "        #pdb.set_trace()\n",
    "        x=self.conv1(x)\n",
    "        x=self.dropout(self.activation(x))\n",
    "        x=x.view(-1)\n",
    "        #print(self.noutput,'after',x.shape)\n",
    "        logits=self.fc1(x)\n",
    "        #print(logits.shape)\n",
    "        #pdb.set_trace()\n",
    "        return logits\n",
    "# print(\"skip this pls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_D(\n",
      "  (layer1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (activation1): LeakyReLU(negative_slope=0.2)\n",
      "  (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): LeakyReLU(negative_slope=0.2)\n",
      "  (layer6): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Seq2Seq2Decoder(\n",
      "  (embedding): Embedding(9603, 256)\n",
      "  (embedding_decoder1): Embedding(9603, 256)\n",
      "  (embedding_decoder2): Embedding(9603, 256)\n",
      "  (encoder): LSTM(256, 256, batch_first=True)\n",
      "  (decoder1): LSTM(512, 256, batch_first=True)\n",
      "  (decoder2): LSTM(512, 256, batch_first=True)\n",
      "  (linear): Linear(in_features=256, out_features=9603, bias=True)\n",
      ")\n",
      "MLP_G(\n",
      "  (layer1): Linear(in_features=64, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): ReLU()\n",
      "  (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): ReLU()\n",
      "  (layer7): Linear(in_features=256, out_features=256, bias=True)\n",
      ")\n",
      "MLP_D(\n",
      "  (layer1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (activation1): LeakyReLU(negative_slope=0.2)\n",
      "  (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): LeakyReLU(negative_slope=0.2)\n",
      "  (layer6): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "MLP_Classify(\n",
      "  (layer1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (activation1): ReLU()\n",
      "  (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): ReLU()\n",
      "  (layer6): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "autoencoder = Seq2Seq2Decoder(emsize=args.emsize,\\\n",
    "                              nhidden=args.nhidden,\\\n",
    "                              ntokens=ntokens,\\\n",
    "                              nlayers=args.nlayers,\\\n",
    "                              noise_r=args.noise_r,\\\n",
    "                              hidden_init=args.hidden_init,\\\n",
    "                              dropout=args.dropout,\\\n",
    "                              gpu=args.cuda)\n",
    "\n",
    "gan_gen = MLP_G(ninput=args.z_size, noutput=args.nhidden, layers=args.arch_g)\n",
    "gan_disc = MLP_D(ninput=args.nhidden, noutput=1, layers=args.arch_d)\n",
    "print(gan_disc)\n",
    "#pdb.set_trace()\n",
    "classifier = MLP_Classify(\n",
    "    ninput=args.nhidden, noutput=1, layers=args.arch_classify)\n",
    "g_factor = None\n",
    "\n",
    "print(autoencoder)\n",
    "print(gan_gen)\n",
    "print(gan_disc)\n",
    "print(classifier)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=args.lr_ae)\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\\\n",
    "                             lr=args.lr_gan_g,\\\n",
    "                             betas=(args.beta1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\\\n",
    "                             lr=args.lr_gan_d,\\\n",
    "                             betas=(args.beta1, 0.999))\n",
    "# classify\n",
    "optimizer_classify = optim.Adam(classifier.parameters(),\\\n",
    "                                lr=args.lr_classify,\\\n",
    "                                betas=(args.beta1, 0.999))\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "if args.cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    gan_gen = gan_gen.cuda()\n",
    "    gan_disc = gan_disc.cuda()\n",
    "    classifier = classifier.cuda()\n",
    "    criterion_ce = criterion_ce.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    print(\"Saving models\")\n",
    "    with open('{}/autoencoder_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(autoencoder.state_dict(), f)\n",
    "    with open('{}/gan_gen_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(gan_gen.state_dict(), f)\n",
    "    with open('{}/gan_disc_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(gan_disc.state_dict(), f)\n",
    "\n",
    "\n",
    "def train_classifier(whichclass, batch):\n",
    "    classifier.train()\n",
    "    classifier.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    labels = to_gpu(args.cuda, Variable(\n",
    "        torch.zeros(source.size(0)).fill_(whichclass - 1)))\n",
    "\n",
    "    # Train\n",
    "    code = autoencoder(0, source, lengths, noise=False,\n",
    "                       encode_only=True).detach()\n",
    "    scores = classifier(code)\n",
    "    classify_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "    classify_loss.backward()\n",
    "    optimizer_classify.step()\n",
    "    classify_loss = classify_loss.cpu().data[0]\n",
    "\n",
    "    pred = scores.data.round().squeeze(1)\n",
    "    accuracy = pred.eq(labels.data).float().mean()\n",
    "\n",
    "    return classify_loss, accuracy\n",
    "\n",
    "\n",
    "def grad_hook_cla(grad):\n",
    "    return grad * args.lambda_class\n",
    "\n",
    "\n",
    "def classifier_regularize(whichclass, batch):\n",
    "    autoencoder.train()\n",
    "    autoencoder.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "    flippedclass = abs(2 - whichclass)\n",
    "    labels = to_gpu(args.cuda, Variable(\n",
    "        torch.zeros(source.size(0)).fill_(flippedclass)))\n",
    "\n",
    "    # Train\n",
    "    code = autoencoder(0, source, lengths, noise=False, encode_only=True)\n",
    "    code.register_hook(grad_hook_cla)\n",
    "    scores = classifier(code)\n",
    "    classify_reg_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "    classify_reg_loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), args.clip)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    return classify_reg_loss\n",
    "\n",
    "\n",
    "def evaluate_autoencoder(whichdecoder, data_source, epoch):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary.word2idx)\n",
    "    all_accuracies = 0\n",
    "    bcnt = 0\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        source = to_gpu(args.cuda, Variable(source, volatile=True))\n",
    "        target = to_gpu(args.cuda, Variable(target, volatile=True))\n",
    "\n",
    "        mask = target.gt(0)\n",
    "        masked_target = target.masked_select(mask)\n",
    "        # examples x ntokens\n",
    "        output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "\n",
    "        hidden = autoencoder(0, source, lengths, noise=False, encode_only=True)\n",
    "\n",
    "        # output: batch x seq_len x ntokens\n",
    "        if whichdecoder == 1:\n",
    "            output = autoencoder(1, source, lengths, noise=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            # accuracy\n",
    "            max_vals1, max_indices1 = torch.max(masked_output, 1)\n",
    "            all_accuracies += \\\n",
    "                torch.mean(max_indices1.eq(masked_target).float()).data[0]\n",
    "\n",
    "            max_values1, max_indices1 = torch.max(output, 2)\n",
    "            max_indices2 = autoencoder.generate(2, hidden, maxlen=50)\n",
    "        else:\n",
    "            output = autoencoder(2, source, lengths, noise=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            # accuracy\n",
    "            max_vals2, max_indices2 = torch.max(masked_output, 1)\n",
    "            all_accuracies += \\\n",
    "                torch.mean(max_indices2.eq(masked_target).float()).data[0]\n",
    "\n",
    "            max_values2, max_indices2 = torch.max(output, 2)\n",
    "            max_indices1 = autoencoder.generate(1, hidden, maxlen=50)\n",
    "\n",
    "        total_loss += criterion_ce(masked_output /\n",
    "                                   args.temp, masked_target).data\n",
    "        bcnt += 1\n",
    "\n",
    "        aeoutf_from = \"{}/{}_output_decoder_{}_from.txt\".format(\n",
    "            args.outf, epoch, whichdecoder)\n",
    "        aeoutf_tran = \"{}/{}_output_decoder_{}_tran.txt\".format(\n",
    "            args.outf, epoch, whichdecoder)\n",
    "        with open(aeoutf_from, 'w') as f_from, open(aeoutf_tran, 'w') as f_trans:\n",
    "            max_indices1 = \\\n",
    "                max_indices1.view(output.size(0), -1).data.cpu().numpy()\n",
    "            max_indices2 = \\\n",
    "                max_indices2.view(output.size(0), -1).data.cpu().numpy()\n",
    "            target = target.view(output.size(0), -1).data.cpu().numpy()\n",
    "            tran_indices = max_indices2 if whichdecoder == 1 else max_indices1\n",
    "            for t, tran_idx in zip(target, tran_indices):\n",
    "                # real sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in t])\n",
    "                f_from.write(chars)\n",
    "                f_from.write(\"\\n\")\n",
    "                # transfer sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x]\n",
    "                                  for x in tran_idx])\n",
    "                f_trans.write(chars)\n",
    "                f_trans.write(\"\\n\")\n",
    "\n",
    "    return total_loss[0] / len(data_source), all_accuracies / bcnt\n",
    "\n",
    "\n",
    "def evaluate_generator(whichdecoder, noise, epoch):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # generate from fixed random noise\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = \\\n",
    "        autoencoder.generate(whichdecoder, fake_hidden,\n",
    "                             maxlen=50, sample=args.sample)\n",
    "\n",
    "    with open(\"%s/%s_generated%d.txt\" % (args.outf, epoch, whichdecoder), \"w\") as f:\n",
    "        max_indices = max_indices.data.cpu().numpy()\n",
    "        for idx in max_indices:\n",
    "            # generated sentence\n",
    "            words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "            # truncate sentences to first occurrence of <eos>\n",
    "            truncated_sent = []\n",
    "            for w in words:\n",
    "                if w != '<eos>':\n",
    "                    truncated_sent.append(w)\n",
    "                else:\n",
    "                    break\n",
    "            chars = \" \".join(truncated_sent)\n",
    "            f.write(chars)\n",
    "            #print(\"word length is:\", len(words))\n",
    "            #print('generated output:\\n', chars)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def train_ae(whichdecoder, batch, total_loss_ae, start_time, i):\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "\n",
    "    mask = target.gt(0)\n",
    "    masked_target = target.masked_select(mask)\n",
    "    output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "    output = autoencoder(whichdecoder, source, lengths, noise=True)\n",
    "    flat_output = output.view(-1, ntokens)\n",
    "    masked_output = flat_output.masked_select(output_mask).view(-1, ntokens)\n",
    "    loss = criterion_ce(masked_output / args.temp, masked_target)\n",
    "    loss.backward()\n",
    "\n",
    "    # `clip_grad_norm` to prevent exploding gradient in RNNs / LSTMs\n",
    "    torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), args.clip)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    total_loss_ae += loss.data\n",
    "\n",
    "    accuracy = None\n",
    "    if i % args.log_interval == 0 and i > 0:\n",
    "        probs = F.softmax(masked_output, dim=-1)\n",
    "        max_vals, max_indices = torch.max(probs, 1)\n",
    "        accuracy = torch.mean(max_indices.eq(masked_target).float()).data[0]\n",
    "        cur_loss = total_loss_ae[0] / args.log_interval\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "              'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}'\n",
    "              .format(epoch, i, len(train1_data),\n",
    "                      elapsed * 1000 / args.log_interval,\n",
    "                      cur_loss, math.exp(cur_loss), accuracy))\n",
    "\n",
    "        with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "            f.write('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}\\n'.\n",
    "                    format(epoch, i, len(train1_data),\n",
    "                           elapsed * 1000 / args.log_interval,\n",
    "                           cur_loss, math.exp(cur_loss), accuracy))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    return total_loss_ae, start_time\n",
    "\n",
    "\n",
    "def train_gan_g():\n",
    "    gan_gen.train()\n",
    "    gan_gen.zero_grad()\n",
    "\n",
    "    noise = to_gpu(args.cuda,Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "    noise.data.normal_(0, 1)\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    errG = gan_disc(fake_hidden)\n",
    "    errG.backward(one)\n",
    "    optimizer_gan_g.step()\n",
    "\n",
    "    return errG\n",
    "\n",
    "\n",
    "def grad_hook(grad):\n",
    "    return grad * args.grad_lambda\n",
    "\n",
    "\n",
    "''' Steal from https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py '''\n",
    "\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    bsz = real_data.size(0)\n",
    "    alpha = torch.rand(bsz, 1)\n",
    "    alpha = alpha.expand(bsz, real_data.size(1))  # only works for 2D XXX\n",
    "    alpha = alpha.cuda()\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\\\n",
    "                                    grad_outputs=torch.ones(\\\n",
    "                                        disc_interpolates.size()).cuda(),\\\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1)\n",
    "                        ** 2).mean() * args.gan_gp_lambda\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def train_gan_d(whichdecoder, batch):\n",
    "    gan_disc.train()\n",
    "    optimizer_gan_d.zero_grad()\n",
    "\n",
    "    # positive samples ----------------------------\n",
    "    # generate real codes\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "\n",
    "    # batch_size x nhidden\n",
    "    real_hidden = autoencoder(whichdecoder, source,lengths, noise=False, encode_only=True)\n",
    "\n",
    "    # loss / backprop\n",
    "    errD_real = gan_disc(real_hidden)\n",
    "    errD_real.backward(one)\n",
    "\n",
    "    # negative samples ----------------------------\n",
    "    # generate fake codes\n",
    "    noise = to_gpu(args.cuda,\n",
    "                Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "    noise.data.normal_(0, 1)\n",
    "\n",
    "    # loss / backprop\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    errD_fake = gan_disc(fake_hidden.detach())\n",
    "    errD_fake.backward(mone)\n",
    "\n",
    "    # gradient penalty\n",
    "    gradient_penalty = calc_gradient_penalty(gan_disc, real_hidden.data, fake_hidden.data)\n",
    "    gradient_penalty.backward()\n",
    "\n",
    "    optimizer_gan_d.step()\n",
    "    errD = -(errD_real.unsqueeze(0) - errD_fake.unsqueeze(0))\n",
    "\n",
    "    return errD, errD_real, errD_fake\n",
    "\n",
    "\n",
    "def train_gan_d_into_ae(whichdecoder, batch):\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "    real_hidden = autoencoder(whichdecoder, source, lengths, noise=False, encode_only=True)\n",
    "    real_hidden.register_hook(grad_hook)\n",
    "    errD_real = gan_disc(real_hidden)\n",
    "    errD_real.backward(mone)\n",
    "    torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), args.clip)\n",
    "\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    return errD_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:99: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:107: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:108: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][10/4176] Loss_D: 4.0297 (Loss_D_real: -3.9153 Loss_D_fake: 0.1144) Loss_G: -0.3106\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][20/4176] Loss_D: 12.5182 (Loss_D_real: -11.1811 Loss_D_fake: 1.3371) Loss_G: -4.3656\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][30/4176] Loss_D: 14.6391 (Loss_D_real: -22.3865 Loss_D_fake: -7.7474) Loss_G: -17.9015\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][40/4176] Loss_D: 23.2796 (Loss_D_real: -29.2576 Loss_D_fake: -5.9780) Loss_G: -19.2714\n",
      "Classify loss:  0.68 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][50/4176] Loss_D: 50.9814 (Loss_D_real: -59.5975 Loss_D_fake: -8.6161) Loss_G: -10.5235\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][60/4176] Loss_D: 68.8028 (Loss_D_real: -73.0244 Loss_D_fake: -4.2217) Loss_G: -14.7705\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][70/4176] Loss_D: 74.8262 (Loss_D_real: -73.3987 Loss_D_fake: 1.4275) Loss_G: -8.8020\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][80/4176] Loss_D: 99.5362 (Loss_D_real: -102.2108 Loss_D_fake: -2.6746) Loss_G: -11.8328\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][90/4176] Loss_D: 99.5146 (Loss_D_real: -61.8344 Loss_D_fake: 37.6803) Loss_G: 38.8163\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][100/4176] Loss_D: 76.2073 (Loss_D_real: -41.6055 Loss_D_fake: 34.6017) Loss_G: 43.9721\n",
      "Classify loss:  0.69 | Classify accuracy: 0.508\n",
      "\n",
      "[1/50][110/4176] Loss_D: 124.3062 (Loss_D_real: -34.9226 Loss_D_fake: 89.3835) Loss_G: 55.1229\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][120/4176] Loss_D: 84.8665 (Loss_D_real: -1.0420 Loss_D_fake: 83.8245) Loss_G: 121.6289\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][130/4176] Loss_D: -69.6699 (Loss_D_real: 141.9082 Loss_D_fake: 72.2383) Loss_G: 52.4163\n",
      "Classify loss:  0.68 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][140/4176] Loss_D: 35.1907 (Loss_D_real: 22.8322 Loss_D_fake: 58.0229) Loss_G: 41.2176\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][150/4176] Loss_D: 42.4950 (Loss_D_real: -41.2108 Loss_D_fake: 1.2841) Loss_G: 8.4593\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][160/4176] Loss_D: 33.2644 (Loss_D_real: -31.7888 Loss_D_fake: 1.4757) Loss_G: -1.6087\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][170/4176] Loss_D: 71.5215 (Loss_D_real: -78.3805 Loss_D_fake: -6.8590) Loss_G: -10.1107\n",
      "Classify loss:  0.68 | Classify accuracy: 0.523\n",
      "\n",
      "[1/50][180/4176] Loss_D: 59.7657 (Loss_D_real: -65.2540 Loss_D_fake: -5.4883) Loss_G: -5.8700\n",
      "Classify loss:  0.68 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][190/4176] Loss_D: 103.7218 (Loss_D_real: -117.5662 Loss_D_fake: -13.8443) Loss_G: -11.9632\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][200/4176] Loss_D: 78.4504 (Loss_D_real: -77.8473 Loss_D_fake: 0.6031) Loss_G: -1.5141\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "| epoch   1 |   200/ 4176 batches | ms/batch 442.61 | loss  5.84 | ppl   344.27 | acc     0.16\n",
      "| epoch   1 |   200/ 4176 batches | ms/batch  0.15 | loss  6.16 | ppl   474.86 | acc     0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:193: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:194: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][210/4176] Loss_D: 131.4359 (Loss_D_real: -97.5576 Loss_D_fake: 33.8782) Loss_G: 29.5398\n",
      "Classify loss:  0.69 | Classify accuracy: 0.555\n",
      "\n",
      "[1/50][220/4176] Loss_D: 121.0372 (Loss_D_real: -70.1016 Loss_D_fake: 50.9356) Loss_G: 49.0350\n",
      "Classify loss:  0.68 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][230/4176] Loss_D: 113.7407 (Loss_D_real: -60.8403 Loss_D_fake: 52.9004) Loss_G: 55.5036\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][240/4176] Loss_D: 125.7127 (Loss_D_real: -59.9161 Loss_D_fake: 65.7966) Loss_G: 31.4634\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][250/4176] Loss_D: 204.9187 (Loss_D_real: -129.8730 Loss_D_fake: 75.0457) Loss_G: 57.3674\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][260/4176] Loss_D: 280.9991 (Loss_D_real: -190.5544 Loss_D_fake: 90.4447) Loss_G: 56.4527\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][270/4176] Loss_D: 404.0694 (Loss_D_real: -220.1509 Loss_D_fake: 183.9185) Loss_G: 56.6594\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][280/4176] Loss_D: 444.5456 (Loss_D_real: -299.5769 Loss_D_fake: 144.9687) Loss_G: 56.2651\n",
      "Classify loss:  0.69 | Classify accuracy: 0.484\n",
      "\n",
      "[1/50][290/4176] Loss_D: 478.6608 (Loss_D_real: -325.2245 Loss_D_fake: 153.4364) Loss_G: 81.6730\n",
      "Classify loss:  0.69 | Classify accuracy: 0.523\n",
      "\n",
      "[1/50][300/4176] Loss_D: 574.8292 (Loss_D_real: -375.5165 Loss_D_fake: 199.3127) Loss_G: 169.3423\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][310/4176] Loss_D: 821.9277 (Loss_D_real: -588.1386 Loss_D_fake: 233.7891) Loss_G: 192.2823\n",
      "Classify loss:  0.69 | Classify accuracy: 0.539\n",
      "\n",
      "[1/50][320/4176] Loss_D: 860.9963 (Loss_D_real: -589.0756 Loss_D_fake: 271.9207) Loss_G: 188.7388\n",
      "Classify loss:  0.69 | Classify accuracy: 0.508\n",
      "\n",
      "[1/50][330/4176] Loss_D: 953.2822 (Loss_D_real: -735.8735 Loss_D_fake: 217.4087) Loss_G: 205.0960\n",
      "Classify loss:  0.69 | Classify accuracy: 0.477\n",
      "\n",
      "[1/50][340/4176] Loss_D: 711.5105 (Loss_D_real: -141.8173 Loss_D_fake: 569.6932) Loss_G: 570.5662\n",
      "Classify loss:  0.69 | Classify accuracy: 0.555\n",
      "\n",
      "[1/50][350/4176] Loss_D: 331.2640 (Loss_D_real: -181.3059 Loss_D_fake: 149.9581) Loss_G: 202.5478\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][360/4176] Loss_D: 459.7250 (Loss_D_real: -184.3406 Loss_D_fake: 275.3844) Loss_G: 31.7351\n",
      "Classify loss:  0.68 | Classify accuracy: 0.945\n",
      "\n",
      "[1/50][370/4176] Loss_D: 589.8873 (Loss_D_real: -244.4088 Loss_D_fake: 345.4785) Loss_G: 323.8104\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][380/4176] Loss_D: 616.4656 (Loss_D_real: -219.7919 Loss_D_fake: 396.6738) Loss_G: 115.8327\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][390/4176] Loss_D: 359.4652 (Loss_D_real: -147.7354 Loss_D_fake: 211.7298) Loss_G: 310.6735\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][400/4176] Loss_D: 719.5240 (Loss_D_real: -194.1784 Loss_D_fake: 525.3456) Loss_G: 331.0219\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "| epoch   1 |   400/ 4176 batches | ms/batch 442.98 | loss  5.09 | ppl   162.14 | acc     0.22\n",
      "| epoch   1 |   400/ 4176 batches | ms/batch  0.15 | loss  5.50 | ppl   244.66 | acc     0.21\n",
      "[1/50][410/4176] Loss_D: 907.7241 (Loss_D_real: -122.8861 Loss_D_fake: 784.8380) Loss_G: 605.6014\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][420/4176] Loss_D: -571.4271 (Loss_D_real: 1259.6204 Loss_D_fake: 688.1933) Loss_G: 625.8813\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][430/4176] Loss_D: -55.2984 (Loss_D_real: 586.8311 Loss_D_fake: 531.5327) Loss_G: 366.0223\n",
      "Classify loss:  0.68 | Classify accuracy: 0.766\n",
      "\n",
      "[1/50][440/4176] Loss_D: -72.9705 (Loss_D_real: 568.9604 Loss_D_fake: 495.9899) Loss_G: 402.1968\n",
      "Classify loss:  0.68 | Classify accuracy: 0.523\n",
      "\n",
      "[1/50][450/4176] Loss_D: 284.7823 (Loss_D_real: 216.7034 Loss_D_fake: 501.4857) Loss_G: 427.9077\n",
      "Classify loss:  0.69 | Classify accuracy: 0.656\n",
      "\n",
      "[1/50][460/4176] Loss_D: -169.2240 (Loss_D_real: 222.2598 Loss_D_fake: 53.0358) Loss_G: 130.6512\n",
      "Classify loss:  0.68 | Classify accuracy: 0.695\n",
      "\n",
      "[1/50][470/4176] Loss_D: 106.0989 (Loss_D_real: 85.1177 Loss_D_fake: 191.2166) Loss_G: 177.9149\n",
      "Classify loss:  0.66 | Classify accuracy: 0.945\n",
      "\n",
      "[1/50][480/4176] Loss_D: 245.6098 (Loss_D_real: -133.2046 Loss_D_fake: 112.4052) Loss_G: 105.9093\n",
      "Classify loss:  0.68 | Classify accuracy: 0.562\n",
      "\n",
      "[1/50][490/4176] Loss_D: 171.0011 (Loss_D_real: -87.1089 Loss_D_fake: 83.8921) Loss_G: 74.7228\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][500/4176] Loss_D: 278.5756 (Loss_D_real: -211.6660 Loss_D_fake: 66.9096) Loss_G: 57.9942\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][510/4176] Loss_D: 350.1693 (Loss_D_real: -220.5519 Loss_D_fake: 129.6174) Loss_G: 120.6937\n",
      "Classify loss:  0.69 | Classify accuracy: 0.523\n",
      "\n",
      "[1/50][520/4176] Loss_D: 824.8207 (Loss_D_real: -622.0527 Loss_D_fake: 202.7679) Loss_G: 191.3524\n",
      "Classify loss:  0.69 | Classify accuracy: 0.469\n",
      "\n",
      "[1/50][530/4176] Loss_D: 739.2795 (Loss_D_real: -469.9711 Loss_D_fake: 269.3084) Loss_G: 234.7659\n",
      "Classify loss:  0.69 | Classify accuracy: 0.609\n",
      "\n",
      "[1/50][540/4176] Loss_D: 398.8983 (Loss_D_real: 118.6049 Loss_D_fake: 517.5032) Loss_G: -51.3294\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "[1/50][550/4176] Loss_D: 509.5237 (Loss_D_real: -317.7482 Loss_D_fake: 191.7755) Loss_G: 184.9636\n",
      "Classify loss:  0.68 | Classify accuracy: 0.625\n",
      "\n",
      "[1/50][560/4176] Loss_D: 31.3049 (Loss_D_real: 145.3013 Loss_D_fake: 176.6063) Loss_G: 141.0424\n",
      "Classify loss:  0.68 | Classify accuracy: 0.625\n",
      "\n",
      "[1/50][570/4176] Loss_D: 573.6667 (Loss_D_real: -477.4142 Loss_D_fake: 96.2525) Loss_G: 111.8923\n",
      "Classify loss:  0.69 | Classify accuracy: 0.664\n",
      "\n",
      "[1/50][580/4176] Loss_D: 1213.7853 (Loss_D_real: -1164.4811 Loss_D_fake: 49.3042) Loss_G: -39.7736\n",
      "Classify loss:  0.70 | Classify accuracy: 0.523\n",
      "\n",
      "[1/50][590/4176] Loss_D: 1368.4670 (Loss_D_real: -1233.3823 Loss_D_fake: 135.0847) Loss_G: -100.6935\n",
      "Classify loss:  0.69 | Classify accuracy: 0.570\n",
      "\n",
      "[1/50][600/4176] Loss_D: 1363.0092 (Loss_D_real: -1207.5219 Loss_D_fake: 155.4873) Loss_G: -804.8845\n",
      "Classify loss:  0.68 | Classify accuracy: 0.547\n",
      "\n",
      "| epoch   1 |   600/ 4176 batches | ms/batch 446.84 | loss  4.80 | ppl   121.66 | acc     0.24\n",
      "| epoch   1 |   600/ 4176 batches | ms/batch  0.15 | loss  5.27 | ppl   194.61 | acc     0.22\n",
      "[1/50][610/4176] Loss_D: 1465.2750 (Loss_D_real: -1436.2180 Loss_D_fake: 29.0570) Loss_G: 9.2859\n",
      "Classify loss:  0.69 | Classify accuracy: 0.617\n",
      "\n",
      "[1/50][620/4176] Loss_D: 1763.0698 (Loss_D_real: -1680.8719 Loss_D_fake: 82.1978) Loss_G: 29.3308\n",
      "Classify loss:  0.69 | Classify accuracy: 0.562\n",
      "\n",
      "[1/50][630/4176] Loss_D: 1696.1355 (Loss_D_real: -1493.7416 Loss_D_fake: 202.3939) Loss_G: -165.9006\n",
      "Classify loss:  0.69 | Classify accuracy: 0.469\n",
      "\n",
      "[1/50][640/4176] Loss_D: 853.3704 (Loss_D_real: -599.4800 Loss_D_fake: 253.8904) Loss_G: 302.9101\n",
      "Classify loss:  0.69 | Classify accuracy: 0.570\n",
      "\n",
      "[1/50][650/4176] Loss_D: 1490.7710 (Loss_D_real: -647.5969 Loss_D_fake: 843.1741) Loss_G: 756.4844\n",
      "Classify loss:  0.70 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][660/4176] Loss_D: 1965.4814 (Loss_D_real: -734.1311 Loss_D_fake: 1231.3503) Loss_G: 1250.0497\n",
      "Classify loss:  0.69 | Classify accuracy: 0.508\n",
      "\n",
      "[1/50][670/4176] Loss_D: 915.9141 (Loss_D_real: -362.4515 Loss_D_fake: 553.4626) Loss_G: 559.9210\n",
      "Classify loss:  0.69 | Classify accuracy: 0.492\n",
      "\n",
      "[1/50][680/4176] Loss_D: -249.6908 (Loss_D_real: 713.2512 Loss_D_fake: 463.5604) Loss_G: 425.7217\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "[1/50][690/4176] Loss_D: -58.5555 (Loss_D_real: 1410.8292 Loss_D_fake: 1352.2737) Loss_G: 1562.4266\n",
      "Classify loss:  0.69 | Classify accuracy: 0.750\n",
      "\n",
      "[1/50][700/4176] Loss_D: 167.7375 (Loss_D_real: 1213.6869 Loss_D_fake: 1381.4244) Loss_G: 1392.9093\n",
      "Classify loss:  0.68 | Classify accuracy: 0.828\n",
      "\n",
      "[1/50][710/4176] Loss_D: -439.4438 (Loss_D_real: 1474.0751 Loss_D_fake: 1034.6312) Loss_G: 423.7568\n",
      "Classify loss:  0.70 | Classify accuracy: 0.352\n",
      "\n",
      "[1/50][720/4176] Loss_D: 299.2366 (Loss_D_real: 869.4181 Loss_D_fake: 1168.6547) Loss_G: 902.4667\n",
      "Classify loss:  0.67 | Classify accuracy: 0.711\n",
      "\n",
      "[1/50][730/4176] Loss_D: 320.0470 (Loss_D_real: 430.5139 Loss_D_fake: 750.5609) Loss_G: 774.2845\n",
      "Classify loss:  0.66 | Classify accuracy: 0.961\n",
      "\n",
      "[1/50][740/4176] Loss_D: 848.8071 (Loss_D_real: -72.1746 Loss_D_fake: 776.6326) Loss_G: 711.9619\n",
      "Classify loss:  0.68 | Classify accuracy: 0.688\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][750/4176] Loss_D: 1516.2988 (Loss_D_real: -679.2322 Loss_D_fake: 837.0666) Loss_G: 695.5833\n",
      "Classify loss:  0.68 | Classify accuracy: 0.695\n",
      "\n",
      "[1/50][760/4176] Loss_D: 36.1835 (Loss_D_real: 399.4016 Loss_D_fake: 435.5851) Loss_G: 230.7465\n",
      "Classify loss:  0.70 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][770/4176] Loss_D: 887.0552 (Loss_D_real: -429.5892 Loss_D_fake: 457.4660) Loss_G: 377.8219\n",
      "Classify loss:  0.68 | Classify accuracy: 0.648\n",
      "\n",
      "[1/50][780/4176] Loss_D: 1104.7777 (Loss_D_real: -654.4812 Loss_D_fake: 450.2965) Loss_G: 292.8434\n",
      "Classify loss:  0.68 | Classify accuracy: 0.594\n",
      "\n",
      "[1/50][790/4176] Loss_D: 1393.5848 (Loss_D_real: -919.8923 Loss_D_fake: 473.6925) Loss_G: 318.3971\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "[1/50][800/4176] Loss_D: 2192.4487 (Loss_D_real: -1854.7424 Loss_D_fake: 337.7063) Loss_G: 222.3363\n",
      "Classify loss:  0.70 | Classify accuracy: 0.469\n",
      "\n",
      "| epoch   1 |   800/ 4176 batches | ms/batch 446.91 | loss  4.56 | ppl    95.85 | acc     0.26\n",
      "| epoch   1 |   800/ 4176 batches | ms/batch  0.15 | loss  5.04 | ppl   155.17 | acc     0.22\n",
      "[1/50][810/4176] Loss_D: 1931.3315 (Loss_D_real: -1541.0939 Loss_D_fake: 390.2377) Loss_G: 323.7522\n",
      "Classify loss:  0.69 | Classify accuracy: 0.484\n",
      "\n",
      "[1/50][820/4176] Loss_D: 2278.7678 (Loss_D_real: -1791.2410 Loss_D_fake: 487.5269) Loss_G: 336.6096\n",
      "Classify loss:  0.69 | Classify accuracy: 0.461\n",
      "\n",
      "[1/50][830/4176] Loss_D: 1756.7362 (Loss_D_real: -406.6335 Loss_D_fake: 1350.1027) Loss_G: 318.8708\n",
      "Classify loss:  0.69 | Classify accuracy: 0.555\n",
      "\n",
      "[1/50][840/4176] Loss_D: 388.8032 (Loss_D_real: -202.7404 Loss_D_fake: 186.0628) Loss_G: 200.7451\n",
      "Classify loss:  0.69 | Classify accuracy: 0.523\n",
      "\n",
      "[1/50][850/4176] Loss_D: 846.7937 (Loss_D_real: -707.1445 Loss_D_fake: 139.6491) Loss_G: 144.0004\n",
      "Classify loss:  0.68 | Classify accuracy: 0.648\n",
      "\n",
      "[1/50][860/4176] Loss_D: 1309.6272 (Loss_D_real: -1145.8942 Loss_D_fake: 163.7330) Loss_G: 162.8221\n",
      "Classify loss:  0.69 | Classify accuracy: 0.469\n",
      "\n",
      "[1/50][870/4176] Loss_D: 2896.5469 (Loss_D_real: -2443.7219 Loss_D_fake: 452.8248) Loss_G: 243.2215\n",
      "Classify loss:  0.69 | Classify accuracy: 0.484\n",
      "\n",
      "[1/50][880/4176] Loss_D: 1694.2227 (Loss_D_real: -1590.8271 Loss_D_fake: 103.3956) Loss_G: -43.8481\n",
      "Classify loss:  0.69 | Classify accuracy: 0.703\n",
      "\n",
      "[1/50][890/4176] Loss_D: 2770.1067 (Loss_D_real: -2541.8840 Loss_D_fake: 228.2228) Loss_G: 227.1392\n",
      "Classify loss:  0.68 | Classify accuracy: 0.594\n",
      "\n",
      "[1/50][900/4176] Loss_D: 3405.8628 (Loss_D_real: -2964.0146 Loss_D_fake: 441.8481) Loss_G: 425.6752\n",
      "Classify loss:  0.69 | Classify accuracy: 0.539\n",
      "\n",
      "[1/50][910/4176] Loss_D: 2726.5525 (Loss_D_real: -2512.4524 Loss_D_fake: 214.1002) Loss_G: 226.2577\n",
      "Classify loss:  0.70 | Classify accuracy: 0.445\n",
      "\n",
      "[1/50][920/4176] Loss_D: 3702.5859 (Loss_D_real: -3273.6670 Loss_D_fake: 428.9190) Loss_G: 314.1276\n",
      "Classify loss:  0.69 | Classify accuracy: 0.562\n",
      "\n",
      "[1/50][930/4176] Loss_D: 3048.9324 (Loss_D_real: -2407.2590 Loss_D_fake: 641.6733) Loss_G: 473.1042\n",
      "Classify loss:  0.69 | Classify accuracy: 0.508\n",
      "\n",
      "[1/50][940/4176] Loss_D: 2577.7383 (Loss_D_real: -1456.5745 Loss_D_fake: 1121.1637) Loss_G: 1038.1930\n",
      "Classify loss:  0.70 | Classify accuracy: 0.477\n",
      "\n",
      "[1/50][950/4176] Loss_D: 2533.8726 (Loss_D_real: -1272.9778 Loss_D_fake: 1260.8949) Loss_G: 725.8788\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][960/4176] Loss_D: 1070.1089 (Loss_D_real: 730.8169 Loss_D_fake: 1800.9258) Loss_G: 1647.7571\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "[1/50][970/4176] Loss_D: 2407.5706 (Loss_D_real: -772.4738 Loss_D_fake: 1635.0968) Loss_G: 1613.3049\n",
      "Classify loss:  0.69 | Classify accuracy: 0.555\n",
      "\n",
      "[1/50][980/4176] Loss_D: 1777.8738 (Loss_D_real: -825.8462 Loss_D_fake: 952.0276) Loss_G: 642.0803\n",
      "Classify loss:  0.67 | Classify accuracy: 0.969\n",
      "\n",
      "[1/50][990/4176] Loss_D: 318.3723 (Loss_D_real: 531.3357 Loss_D_fake: 849.7080) Loss_G: 574.5754\n",
      "Classify loss:  0.68 | Classify accuracy: 0.641\n",
      "\n",
      "[1/50][1000/4176] Loss_D: 916.0717 (Loss_D_real: -254.3965 Loss_D_fake: 661.6751) Loss_G: 608.6949\n",
      "Classify loss:  0.66 | Classify accuracy: 0.922\n",
      "\n",
      "| epoch   1 |  1000/ 4176 batches | ms/batch 447.06 | loss  4.40 | ppl    81.40 | acc     0.27\n",
      "| epoch   1 |  1000/ 4176 batches | ms/batch  0.15 | loss  4.87 | ppl   129.78 | acc     0.23\n",
      "[1/50][1010/4176] Loss_D: 464.3892 (Loss_D_real: -811.9604 Loss_D_fake: -347.5712) Loss_G: 336.9776\n",
      "Classify loss:  0.66 | Classify accuracy: 0.898\n",
      "\n",
      "[1/50][1020/4176] Loss_D: 1123.7156 (Loss_D_real: -765.3883 Loss_D_fake: 358.3273) Loss_G: 1032.2574\n",
      "Classify loss:  0.66 | Classify accuracy: 0.688\n",
      "\n",
      "[1/50][1030/4176] Loss_D: 812.5822 (Loss_D_real: -738.1277 Loss_D_fake: 74.4544) Loss_G: 71.0785\n",
      "Classify loss:  0.67 | Classify accuracy: 0.703\n",
      "\n",
      "[1/50][1040/4176] Loss_D: 1385.5604 (Loss_D_real: -1331.5472 Loss_D_fake: 54.0132) Loss_G: 126.7296\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "[1/50][1050/4176] Loss_D: 1798.1943 (Loss_D_real: -1665.8419 Loss_D_fake: 132.3524) Loss_G: 133.7693\n",
      "Classify loss:  0.68 | Classify accuracy: 0.594\n",
      "\n",
      "[1/50][1060/4176] Loss_D: 3143.9341 (Loss_D_real: -2801.5959 Loss_D_fake: 342.3383) Loss_G: 292.9938\n",
      "Classify loss:  0.67 | Classify accuracy: 0.562\n",
      "\n",
      "[1/50][1070/4176] Loss_D: 4129.8940 (Loss_D_real: -3139.9045 Loss_D_fake: 989.9896) Loss_G: -357.0426\n",
      "Classify loss:  0.69 | Classify accuracy: 0.484\n",
      "\n",
      "[1/50][1080/4176] Loss_D: 1969.6859 (Loss_D_real: -2263.1990 Loss_D_fake: -293.5131) Loss_G: -359.1517\n",
      "Classify loss:  0.67 | Classify accuracy: 0.617\n",
      "\n",
      "[1/50][1090/4176] Loss_D: 2452.2759 (Loss_D_real: -2849.2927 Loss_D_fake: -397.0168) Loss_G: -361.8464\n",
      "Classify loss:  0.67 | Classify accuracy: 0.750\n",
      "\n",
      "[1/50][1100/4176] Loss_D: 3157.9590 (Loss_D_real: -3178.3596 Loss_D_fake: -20.4007) Loss_G: -152.5744\n",
      "Classify loss:  0.67 | Classify accuracy: 0.789\n",
      "\n",
      "[1/50][1110/4176] Loss_D: 4003.5454 (Loss_D_real: -4183.8286 Loss_D_fake: -180.2831) Loss_G: -201.8397\n",
      "Classify loss:  0.66 | Classify accuracy: 0.672\n",
      "\n",
      "[1/50][1120/4176] Loss_D: 4357.1211 (Loss_D_real: -4265.7178 Loss_D_fake: 91.4035) Loss_G: 75.7750\n",
      "Classify loss:  0.69 | Classify accuracy: 0.492\n",
      "\n",
      "[1/50][1130/4176] Loss_D: 4834.2432 (Loss_D_real: -4843.9219 Loss_D_fake: -9.6786) Loss_G: -68.5747\n",
      "Classify loss:  0.68 | Classify accuracy: 0.617\n",
      "\n",
      "[1/50][1140/4176] Loss_D: 4913.3745 (Loss_D_real: -4548.1230 Loss_D_fake: 365.2515) Loss_G: -253.5324\n",
      "Classify loss:  0.67 | Classify accuracy: 0.680\n",
      "\n",
      "[1/50][1150/4176] Loss_D: 5080.9565 (Loss_D_real: -4929.2852 Loss_D_fake: 151.6716) Loss_G: 121.8567\n",
      "Classify loss:  0.67 | Classify accuracy: 0.633\n",
      "\n",
      "[1/50][1160/4176] Loss_D: 5127.8071 (Loss_D_real: -4672.1313 Loss_D_fake: 455.6758) Loss_G: 457.5700\n",
      "Classify loss:  0.69 | Classify accuracy: 0.547\n",
      "\n",
      "[1/50][1170/4176] Loss_D: 6085.9619 (Loss_D_real: -5121.8804 Loss_D_fake: 964.0815) Loss_G: -517.8499\n",
      "Classify loss:  0.66 | Classify accuracy: 0.773\n",
      "\n",
      "[1/50][1180/4176] Loss_D: 4883.0498 (Loss_D_real: -4462.7041 Loss_D_fake: 420.3458) Loss_G: 476.9072\n",
      "Classify loss:  0.68 | Classify accuracy: 0.578\n",
      "\n",
      "[1/50][1190/4176] Loss_D: 6164.4556 (Loss_D_real: -5335.7280 Loss_D_fake: 828.7274) Loss_G: 527.6779\n",
      "Classify loss:  0.65 | Classify accuracy: 0.719\n",
      "\n",
      "[1/50][1200/4176] Loss_D: 6084.8403 (Loss_D_real: -5328.6226 Loss_D_fake: 756.2176) Loss_G: -1457.8678\n",
      "Classify loss:  0.69 | Classify accuracy: 0.555\n",
      "\n",
      "| epoch   1 |  1200/ 4176 batches | ms/batch 447.17 | loss  4.27 | ppl    71.57 | acc     0.30\n",
      "| epoch   1 |  1200/ 4176 batches | ms/batch  0.15 | loss  4.77 | ppl   117.34 | acc     0.26\n",
      "[1/50][1210/4176] Loss_D: 6524.9014 (Loss_D_real: -5985.0396 Loss_D_fake: 539.8618) Loss_G: 333.1085\n",
      "Classify loss:  0.68 | Classify accuracy: 0.633\n",
      "\n",
      "[1/50][1220/4176] Loss_D: 5213.1392 (Loss_D_real: -4594.5176 Loss_D_fake: 618.6216) Loss_G: 290.7927\n",
      "Classify loss:  0.68 | Classify accuracy: 0.695\n",
      "\n",
      "[1/50][1230/4176] Loss_D: 6864.6504 (Loss_D_real: -5935.6357 Loss_D_fake: 929.0146) Loss_G: 107.4011\n",
      "Classify loss:  0.68 | Classify accuracy: 0.586\n",
      "\n",
      "[1/50][1240/4176] Loss_D: 6858.3550 (Loss_D_real: -6107.8306 Loss_D_fake: 750.5245) Loss_G: 380.2532\n",
      "Classify loss:  0.67 | Classify accuracy: 0.828\n",
      "\n",
      "[1/50][1250/4176] Loss_D: 4835.8960 (Loss_D_real: -4251.7954 Loss_D_fake: 584.1007) Loss_G: 411.0090\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "[1/50][1260/4176] Loss_D: 1854.4185 (Loss_D_real: -318.0742 Loss_D_fake: 1536.3442) Loss_G: 1443.8163\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][1270/4176] Loss_D: 2546.8042 (Loss_D_real: -484.1740 Loss_D_fake: 2062.6301) Loss_G: 2253.8030\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][1280/4176] Loss_D: 3638.4189 (Loss_D_real: -565.3092 Loss_D_fake: 3073.1096) Loss_G: 2994.2578\n",
      "Classify loss:  0.71 | Classify accuracy: 0.461\n",
      "\n",
      "[1/50][1290/4176] Loss_D: 5472.1689 (Loss_D_real: -2106.3506 Loss_D_fake: 3365.8181) Loss_G: 2930.7759\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][1300/4176] Loss_D: 5032.8447 (Loss_D_real: -1028.5142 Loss_D_fake: 4004.3303) Loss_G: 1556.0028\n",
      "Classify loss:  0.70 | Classify accuracy: 0.492\n",
      "\n",
      "[1/50][1310/4176] Loss_D: 5823.7900 (Loss_D_real: -3213.8328 Loss_D_fake: 2609.9570) Loss_G: 1252.3234\n",
      "Classify loss:  0.69 | Classify accuracy: 0.508\n",
      "\n",
      "[1/50][1320/4176] Loss_D: -2210.3430 (Loss_D_real: 4416.5566 Loss_D_fake: 2206.2136) Loss_G: 244.7981\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][1330/4176] Loss_D: 760.2380 (Loss_D_real: 2771.4307 Loss_D_fake: 3531.6687) Loss_G: 3486.7126\n",
      "Classify loss:  0.69 | Classify accuracy: 0.508\n",
      "\n",
      "[1/50][1340/4176] Loss_D: -1567.3356 (Loss_D_real: 2492.7161 Loss_D_fake: 925.3805) Loss_G: 1022.2867\n",
      "Classify loss:  0.69 | Classify accuracy: 0.547\n",
      "\n",
      "[1/50][1350/4176] Loss_D: -344.3258 (Loss_D_real: 1559.6774 Loss_D_fake: 1215.3516) Loss_G: 933.9613\n",
      "Classify loss:  0.67 | Classify accuracy: 0.617\n",
      "\n",
      "[1/50][1360/4176] Loss_D: -614.8775 (Loss_D_real: 1365.7384 Loss_D_fake: 750.8609) Loss_G: 750.3669\n",
      "Classify loss:  0.65 | Classify accuracy: 0.805\n",
      "\n",
      "[1/50][1370/4176] Loss_D: 272.3825 (Loss_D_real: 191.6690 Loss_D_fake: 464.0515) Loss_G: 649.7659\n",
      "Classify loss:  0.67 | Classify accuracy: 0.672\n",
      "\n",
      "[1/50][1380/4176] Loss_D: 1397.3496 (Loss_D_real: -906.7986 Loss_D_fake: 490.5509) Loss_G: 381.4720\n",
      "Classify loss:  0.71 | Classify accuracy: 0.336\n",
      "\n",
      "[1/50][1390/4176] Loss_D: 867.0612 (Loss_D_real: -786.7491 Loss_D_fake: 80.3121) Loss_G: 82.5816\n",
      "Classify loss:  0.69 | Classify accuracy: 0.539\n",
      "\n",
      "[1/50][1400/4176] Loss_D: 1705.9863 (Loss_D_real: -1495.9207 Loss_D_fake: 210.0656) Loss_G: 149.9198\n",
      "Classify loss:  0.68 | Classify accuracy: 0.805\n",
      "\n",
      "| epoch   1 |  1400/ 4176 batches | ms/batch 446.92 | loss  4.21 | ppl    67.12 | acc     0.29\n",
      "| epoch   1 |  1400/ 4176 batches | ms/batch  0.16 | loss  4.67 | ppl   106.57 | acc     0.24\n",
      "[1/50][1410/4176] Loss_D: 1632.2484 (Loss_D_real: -2010.8805 Loss_D_fake: -378.6321) Loss_G: -5.7589\n",
      "Classify loss:  0.69 | Classify accuracy: 0.484\n",
      "\n",
      "[1/50][1420/4176] Loss_D: 2141.6660 (Loss_D_real: -2750.2827 Loss_D_fake: -608.6167) Loss_G: -783.8110\n",
      "Classify loss:  0.69 | Classify accuracy: 0.539\n",
      "\n",
      "[1/50][1430/4176] Loss_D: 2321.6855 (Loss_D_real: -3166.1934 Loss_D_fake: -844.5079) Loss_G: -923.5135\n",
      "Classify loss:  0.68 | Classify accuracy: 0.695\n",
      "\n",
      "[1/50][1440/4176] Loss_D: 3642.4236 (Loss_D_real: -4898.1934 Loss_D_fake: -1255.7698) Loss_G: -1126.2042\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "[1/50][1450/4176] Loss_D: 3804.6047 (Loss_D_real: -4966.2295 Loss_D_fake: -1161.6248) Loss_G: -1090.5006\n",
      "Classify loss:  0.67 | Classify accuracy: 0.734\n",
      "\n",
      "[1/50][1460/4176] Loss_D: 5120.9297 (Loss_D_real: -6207.2056 Loss_D_fake: -1086.2758) Loss_G: -993.0890\n",
      "Classify loss:  0.69 | Classify accuracy: 0.461\n",
      "\n",
      "[1/50][1470/4176] Loss_D: 5434.7349 (Loss_D_real: -6174.7603 Loss_D_fake: -740.0253) Loss_G: -859.1441\n",
      "Classify loss:  0.70 | Classify accuracy: 0.453\n",
      "\n",
      "[1/50][1480/4176] Loss_D: 6471.3154 (Loss_D_real: -7161.0918 Loss_D_fake: -689.7765) Loss_G: -708.5783\n",
      "Classify loss:  0.69 | Classify accuracy: 0.578\n",
      "\n",
      "[1/50][1490/4176] Loss_D: 6866.8091 (Loss_D_real: -7082.8501 Loss_D_fake: -216.0408) Loss_G: -191.5873\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "[1/50][1500/4176] Loss_D: 8523.8555 (Loss_D_real: -8481.8535 Loss_D_fake: 42.0024) Loss_G: -14.8554\n",
      "Classify loss:  0.68 | Classify accuracy: 0.820\n",
      "\n",
      "[1/50][1510/4176] Loss_D: 7054.4570 (Loss_D_real: -6994.1392 Loss_D_fake: 60.3178) Loss_G: -528.5591\n",
      "Classify loss:  0.66 | Classify accuracy: 0.789\n",
      "\n",
      "[1/50][1520/4176] Loss_D: 6166.4507 (Loss_D_real: -5814.9761 Loss_D_fake: 351.4746) Loss_G: 224.5760\n",
      "Classify loss:  0.68 | Classify accuracy: 0.664\n",
      "\n",
      "[1/50][1530/4176] Loss_D: 5393.7075 (Loss_D_real: -2595.4541 Loss_D_fake: 2798.2534) Loss_G: -2527.3462\n",
      "Classify loss:  0.68 | Classify accuracy: 0.523\n",
      "\n",
      "[1/50][1540/4176] Loss_D: 5626.8047 (Loss_D_real: -6007.6392 Loss_D_fake: -380.8346) Loss_G: -4589.7339\n",
      "Classify loss:  0.68 | Classify accuracy: 0.555\n",
      "\n",
      "[1/50][1550/4176] Loss_D: 7031.9399 (Loss_D_real: -7037.9805 Loss_D_fake: -6.0407) Loss_G: -770.8813\n",
      "Classify loss:  0.64 | Classify accuracy: 0.820\n",
      "\n",
      "[1/50][1560/4176] Loss_D: 5894.4858 (Loss_D_real: -5219.2949 Loss_D_fake: 675.1910) Loss_G: -2447.3250\n",
      "Classify loss:  0.68 | Classify accuracy: 0.578\n",
      "\n",
      "[1/50][1570/4176] Loss_D: 8754.1904 (Loss_D_real: -8882.6621 Loss_D_fake: -128.4715) Loss_G: -366.8021\n",
      "Classify loss:  0.64 | Classify accuracy: 0.953\n",
      "\n",
      "[1/50][1580/4176] Loss_D: 9050.1074 (Loss_D_real: -8741.2959 Loss_D_fake: 308.8110) Loss_G: -964.8582\n",
      "Classify loss:  0.69 | Classify accuracy: 0.539\n",
      "\n",
      "[1/50][1590/4176] Loss_D: 8181.7700 (Loss_D_real: -8771.3838 Loss_D_fake: -589.6136) Loss_G: -605.5545\n",
      "Classify loss:  0.71 | Classify accuracy: 0.430\n",
      "\n",
      "[1/50][1600/4176] Loss_D: 8016.7046 (Loss_D_real: -8041.9092 Loss_D_fake: -25.2044) Loss_G: -362.8576\n",
      "Classify loss:  0.70 | Classify accuracy: 0.328\n",
      "\n",
      "| epoch   1 |  1600/ 4176 batches | ms/batch 445.25 | loss  4.12 | ppl    61.59 | acc     0.31\n",
      "| epoch   1 |  1600/ 4176 batches | ms/batch  0.15 | loss  4.60 | ppl    99.94 | acc     0.27\n",
      "[1/50][1610/4176] Loss_D: 1565.4248 (Loss_D_real: -1170.4987 Loss_D_fake: 394.9261) Loss_G: 475.4041\n",
      "Classify loss:  0.64 | Classify accuracy: 0.812\n",
      "\n",
      "[1/50][1620/4176] Loss_D: 8627.7812 (Loss_D_real: -7604.0142 Loss_D_fake: 1023.7669) Loss_G: 1511.3533\n",
      "Classify loss:  0.70 | Classify accuracy: 0.492\n",
      "\n",
      "[1/50][1630/4176] Loss_D: 5584.4424 (Loss_D_real: -3557.8027 Loss_D_fake: 2026.6398) Loss_G: 2244.7778\n",
      "Classify loss:  0.69 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][1640/4176] Loss_D: 7911.7793 (Loss_D_real: -4836.0776 Loss_D_fake: 3075.7019) Loss_G: 2543.4070\n",
      "Classify loss:  0.70 | Classify accuracy: 0.492\n",
      "\n",
      "[1/50][1650/4176] Loss_D: 9018.8174 (Loss_D_real: -5304.1626 Loss_D_fake: 3714.6548) Loss_G: 3729.0688\n",
      "Classify loss:  0.68 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][1660/4176] Loss_D: 8197.2070 (Loss_D_real: -5225.4082 Loss_D_fake: 2971.7983) Loss_G: 2598.1768\n",
      "Classify loss:  0.69 | Classify accuracy: 0.508\n",
      "\n",
      "[1/50][1670/4176] Loss_D: 2679.0156 (Loss_D_real: 1060.0804 Loss_D_fake: 3739.0959) Loss_G: 4240.0659\n",
      "Classify loss:  0.70 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][1680/4176] Loss_D: 1239.3147 (Loss_D_real: 3789.9729 Loss_D_fake: 5029.2876) Loss_G: 4715.2153\n",
      "Classify loss:  0.68 | Classify accuracy: 0.492\n",
      "\n",
      "[1/50][1690/4176] Loss_D: -656.9443 (Loss_D_real: 4989.1343 Loss_D_fake: 4332.1899) Loss_G: 4456.2192\n",
      "Classify loss:  0.66 | Classify accuracy: 0.547\n",
      "\n",
      "[1/50][1700/4176] Loss_D: 4141.0415 (Loss_D_real: 316.2950 Loss_D_fake: 4457.3364) Loss_G: 4435.0063\n",
      "Classify loss:  0.68 | Classify accuracy: 0.547\n",
      "\n",
      "[1/50][1710/4176] Loss_D: 3379.4355 (Loss_D_real: 1373.2070 Loss_D_fake: 4752.6426) Loss_G: 4798.1431\n",
      "Classify loss:  0.64 | Classify accuracy: 0.883\n",
      "\n",
      "[1/50][1720/4176] Loss_D: 3018.0913 (Loss_D_real: 1711.3600 Loss_D_fake: 4729.4512) Loss_G: 4469.2271\n",
      "Classify loss:  0.63 | Classify accuracy: 0.898\n",
      "\n",
      "[1/50][1730/4176] Loss_D: 757.9279 (Loss_D_real: 1228.0634 Loss_D_fake: 1985.9912) Loss_G: 1300.7551\n",
      "Classify loss:  0.63 | Classify accuracy: 0.711\n",
      "\n",
      "[1/50][1740/4176] Loss_D: 1005.5480 (Loss_D_real: 1466.2213 Loss_D_fake: 2471.7693) Loss_G: 2434.2249\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "[1/50][1750/4176] Loss_D: 2243.2261 (Loss_D_real: 395.6981 Loss_D_fake: 2638.9241) Loss_G: 572.9191\n",
      "Classify loss:  0.63 | Classify accuracy: 0.969\n",
      "\n",
      "[1/50][1760/4176] Loss_D: 3994.4595 (Loss_D_real: -1307.3391 Loss_D_fake: 2687.1204) Loss_G: 3123.1543\n",
      "Classify loss:  0.61 | Classify accuracy: 0.938\n",
      "\n",
      "[1/50][1770/4176] Loss_D: 3462.0559 (Loss_D_real: -682.1162 Loss_D_fake: 2779.9397) Loss_G: 2589.0471\n",
      "Classify loss:  0.66 | Classify accuracy: 1.000\n",
      "\n",
      "[1/50][1780/4176] Loss_D: 3143.0198 (Loss_D_real: -814.6313 Loss_D_fake: 2328.3884) Loss_G: 2473.7253\n",
      "Classify loss:  0.60 | Classify accuracy: 0.516\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][1790/4176] Loss_D: 2741.2307 (Loss_D_real: -79.3857 Loss_D_fake: 2661.8450) Loss_G: 2567.1675\n",
      "Classify loss:  0.69 | Classify accuracy: 0.469\n",
      "\n",
      "[1/50][1800/4176] Loss_D: 3203.6658 (Loss_D_real: -734.6635 Loss_D_fake: 2469.0022) Loss_G: 2177.8140\n",
      "Classify loss:  0.69 | Classify accuracy: 0.516\n",
      "\n",
      "| epoch   1 |  1800/ 4176 batches | ms/batch 446.11 | loss  4.07 | ppl    58.30 | acc     0.30\n",
      "| epoch   1 |  1800/ 4176 batches | ms/batch  0.15 | loss  4.56 | ppl    95.32 | acc     0.27\n",
      "[1/50][1810/4176] Loss_D: 5467.2397 (Loss_D_real: -2889.9719 Loss_D_fake: 2577.2678) Loss_G: 2607.4836\n",
      "Classify loss:  0.68 | Classify accuracy: 0.500\n",
      "\n",
      "[1/50][1820/4176] Loss_D: 10065.9570 (Loss_D_real: -5495.3726 Loss_D_fake: 4570.5840) Loss_G: 724.8710\n",
      "Classify loss:  0.67 | Classify accuracy: 0.711\n",
      "\n",
      "[1/50][1830/4176] Loss_D: 5637.0361 (Loss_D_real: -2330.4451 Loss_D_fake: 3306.5908) Loss_G: 2317.3362\n",
      "Classify loss:  0.63 | Classify accuracy: 0.773\n",
      "\n",
      "[1/50][1840/4176] Loss_D: 2572.8977 (Loss_D_real: -2535.9387 Loss_D_fake: 36.9589) Loss_G: -2025.4397\n",
      "Classify loss:  0.65 | Classify accuracy: 0.828\n",
      "\n",
      "[1/50][1850/4176] Loss_D: 5851.4297 (Loss_D_real: -4169.1729 Loss_D_fake: 1682.2567) Loss_G: -1866.6495\n",
      "Classify loss:  0.68 | Classify accuracy: 0.711\n",
      "\n",
      "[1/50][1860/4176] Loss_D: 5304.3149 (Loss_D_real: -4697.3691 Loss_D_fake: 606.9457) Loss_G: 27.2504\n",
      "Classify loss:  0.62 | Classify accuracy: 0.961\n",
      "\n",
      "[1/50][1870/4176] Loss_D: 5205.2612 (Loss_D_real: -4356.2622 Loss_D_fake: 848.9991) Loss_G: 389.4824\n",
      "Classify loss:  0.67 | Classify accuracy: 0.578\n",
      "\n",
      "[1/50][1880/4176] Loss_D: 7048.6836 (Loss_D_real: -6875.1714 Loss_D_fake: 173.5122) Loss_G: 119.2691\n",
      "Classify loss:  0.66 | Classify accuracy: 0.672\n",
      "\n",
      "[1/50][1890/4176] Loss_D: 8103.2070 (Loss_D_real: -7949.6201 Loss_D_fake: 153.5871) Loss_G: 262.6198\n",
      "Classify loss:  0.69 | Classify accuracy: 0.555\n",
      "\n",
      "[1/50][1900/4176] Loss_D: 8097.3657 (Loss_D_real: -7606.2573 Loss_D_fake: 491.1085) Loss_G: -347.9807\n",
      "Classify loss:  0.68 | Classify accuracy: 0.539\n",
      "\n",
      "[1/50][1910/4176] Loss_D: 9322.4805 (Loss_D_real: -8133.8652 Loss_D_fake: 1188.6157) Loss_G: 730.7255\n",
      "Classify loss:  0.69 | Classify accuracy: 0.562\n",
      "\n",
      "[1/50][1920/4176] Loss_D: 9000.0645 (Loss_D_real: -8065.9062 Loss_D_fake: 934.1587) Loss_G: 500.5068\n",
      "Classify loss:  0.70 | Classify accuracy: 0.445\n",
      "\n",
      "[1/50][1930/4176] Loss_D: 9399.4961 (Loss_D_real: -7595.5444 Loss_D_fake: 1803.9521) Loss_G: -3020.1350\n",
      "Classify loss:  0.69 | Classify accuracy: 0.555\n",
      "\n",
      "[1/50][1940/4176] Loss_D: 8886.8594 (Loss_D_real: -7975.6616 Loss_D_fake: 911.1980) Loss_G: 531.1903\n",
      "Classify loss:  0.69 | Classify accuracy: 0.570\n",
      "\n",
      "[1/50][1950/4176] Loss_D: 8995.9619 (Loss_D_real: -8558.1895 Loss_D_fake: 437.7725) Loss_G: 406.5045\n",
      "Classify loss:  0.70 | Classify accuracy: 0.492\n",
      "\n",
      "[1/50][1960/4176] Loss_D: 6129.4961 (Loss_D_real: -5047.2476 Loss_D_fake: 1082.2484) Loss_G: 873.6552\n",
      "Classify loss:  0.67 | Classify accuracy: 0.781\n",
      "\n",
      "[1/50][1970/4176] Loss_D: 5389.3965 (Loss_D_real: -3102.1716 Loss_D_fake: 2287.2251) Loss_G: 2186.0894\n",
      "Classify loss:  0.67 | Classify accuracy: 0.906\n",
      "\n",
      "[1/50][1980/4176] Loss_D: 3403.5706 (Loss_D_real: -3702.4207 Loss_D_fake: -298.8500) Loss_G: 451.2847\n",
      "Classify loss:  0.68 | Classify accuracy: 0.562\n",
      "\n",
      "[1/50][1990/4176] Loss_D: 9160.1826 (Loss_D_real: -8017.9980 Loss_D_fake: 1142.1844) Loss_G: 1663.9573\n",
      "Classify loss:  0.67 | Classify accuracy: 0.508\n",
      "\n",
      "[1/50][2000/4176] Loss_D: 4819.3525 (Loss_D_real: -4624.2915 Loss_D_fake: 195.0610) Loss_G: -5246.2520\n",
      "Classify loss:  0.69 | Classify accuracy: 0.547\n",
      "\n",
      "| epoch   1 |  2000/ 4176 batches | ms/batch 445.84 | loss  4.03 | ppl    56.48 | acc     0.31\n",
      "| epoch   1 |  2000/ 4176 batches | ms/batch  0.15 | loss  4.49 | ppl    89.51 | acc     0.26\n",
      "[1/50][2010/4176] Loss_D: 8967.4893 (Loss_D_real: -2976.1404 Loss_D_fake: 5991.3486) Loss_G: -2498.6123\n",
      "Classify loss:  0.69 | Classify accuracy: 0.531\n",
      "\n",
      "[1/50][2020/4176] Loss_D: 1787.4537 (Loss_D_real: 788.8690 Loss_D_fake: 2576.3228) Loss_G: 2729.3447\n",
      "Classify loss:  0.70 | Classify accuracy: 0.477\n",
      "\n",
      "[1/50][2030/4176] Loss_D: 6874.5186 (Loss_D_real: -4601.4980 Loss_D_fake: 2273.0205) Loss_G: 2682.1401\n",
      "Classify loss:  0.71 | Classify accuracy: 0.391\n",
      "\n",
      "[1/50][2040/4176] Loss_D: 3490.5439 (Loss_D_real: 1431.6498 Loss_D_fake: 4922.1938) Loss_G: -1267.1779\n",
      "Classify loss:  0.69 | Classify accuracy: 0.531\n",
      "\n",
      "[1/50][2050/4176] Loss_D: -1306.9521 (Loss_D_real: -309.5068 Loss_D_fake: -1616.4590) Loss_G: -1616.3823\n",
      "Classify loss:  0.67 | Classify accuracy: 0.555\n",
      "\n",
      "[1/50][2060/4176] Loss_D: 1015.0781 (Loss_D_real: -1026.9817 Loss_D_fake: -11.9036) Loss_G: -1108.5385\n",
      "Classify loss:  0.65 | Classify accuracy: 0.773\n",
      "\n",
      "[1/50][2070/4176] Loss_D: 3348.4043 (Loss_D_real: -3671.6597 Loss_D_fake: -323.2555) Loss_G: -726.7836\n",
      "Classify loss:  0.68 | Classify accuracy: 0.586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "    f.write('Training...\\n')\n",
    "\n",
    "# schedule of increasing GAN training loops\n",
    "if args.niters_gan_schedule != \"\":\n",
    "    gan_schedule = [int(x) for x in args.niters_gan_schedule.split(\"-\")]\n",
    "else:\n",
    "    gan_schedule = []\n",
    "niter_gan = 25\n",
    "\n",
    "fixed_noise = to_gpu(args.cuda,Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "fixed_noise.data.normal_(0, 1)\n",
    "one = Variable(torch.tensor(1.0).cuda())#to_gpu(args.cuda, torch.FloatTensor([1]))\n",
    "mone = Variable(torch.tensor(-1.0).cuda())#one * -1\n",
    "#one = Variable(one, requires_grad=True).cuda()#torch.tensor(1.0, dtype=torch.float64,device=torch.device('cuda:0'))\n",
    "#mone = #Variable(mone, requires_grad=True).cuda()\n",
    "#mone = torch.tensor(-1.0, dtype=torch.float64,device=torch.device('cuda:0'))\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    # update gan training schedule\n",
    "    if epoch in gan_schedule:\n",
    "        niter_gan += 1\n",
    "        print(\"GAN training loop schedule increased to {}\".format(niter_gan))\n",
    "        with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "            f.write(\"GAN training loop schedule increased to {}\\n\".\n",
    "                    format(niter_gan))\n",
    "\n",
    "    total_loss_ae1 = 0\n",
    "    total_loss_ae2 = 0\n",
    "    classify_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "    start_time = time.time()\n",
    "    niter = 0\n",
    "    niter_global = 1\n",
    "\n",
    "    # loop through all batches in training data\n",
    "    while niter < len(train1_data) and niter < len(train2_data):\n",
    "\n",
    "        # train autoencoder ----------------------------\n",
    "        for i in range(args.niters_ae):\n",
    "            if niter == len(train1_data):\n",
    "                break  # end of epoch\n",
    "            total_loss_ae1, start_time = train_ae(1, train1_data[niter], total_loss_ae1, start_time, niter)\n",
    "            total_loss_ae2, _ = train_ae(2, train2_data[niter], total_loss_ae2, start_time, niter)\n",
    "\n",
    "            # train classifier ----------------------------\n",
    "            classify_loss1, classify_acc1 = train_classifier(\n",
    "                1, train1_data[niter])\n",
    "            classify_loss2, classify_acc2 = train_classifier(\n",
    "                2, train2_data[niter])\n",
    "            classify_loss = (classify_loss1 + classify_loss2) / 2\n",
    "            classify_acc = (classify_acc1 + classify_acc2) / 2\n",
    "            # reverse to autoencoder\n",
    "            classifier_regularize(1, train1_data[niter])\n",
    "            classifier_regularize(2, train2_data[niter])\n",
    "\n",
    "            niter += 1\n",
    "\n",
    "        # train gan ----------------------------------\n",
    "        for k in range(niter_gan):\n",
    "\n",
    "            # train discriminator/critic\n",
    "            for i in range(args.niters_gan_d):\n",
    "                # feed a seen sample within this epoch; good for early training\n",
    "                if i % 2 == 0:\n",
    "                    batch = train1_data[\n",
    "                        random.randint(0, len(train1_data) - 1)]\n",
    "                    whichdecoder = 1\n",
    "                else:\n",
    "                    batch = train2_data[\n",
    "                        random.randint(0, len(train2_data) - 1)]\n",
    "                    whichdecoder = 2\n",
    "                errD, errD_real, errD_fake = train_gan_d(whichdecoder, batch)\n",
    "\n",
    "            # train generator\n",
    "            for i in range(args.niters_gan_g):\n",
    "                errG = train_gan_g()\n",
    "\n",
    "            # train autoencoder from d\n",
    "            for i in range(args.niters_gan_ae):\n",
    "                if i % 2 == 0:\n",
    "                    batch = train1_data[\n",
    "                        random.randint(0, len(train1_data) - 1)]\n",
    "                    whichdecoder = 1\n",
    "                else:\n",
    "                    batch = train2_data[\n",
    "                        random.randint(0, len(train2_data) - 1)]\n",
    "                    whichdecoder = 2\n",
    "                errD_= train_gan_d_into_ae(whichdecoder, batch)\n",
    "\n",
    "        niter_global += 1\n",
    "        if niter_global % 100 >= 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f (Loss_D_real: %.4f '\n",
    "                  'Loss_D_fake: %.4f) Loss_G: %.4f'\n",
    "                  % (epoch, args.epochs, niter, len(train1_data),\n",
    "                     errD.data[0], errD_real.data[0],\n",
    "                     errD_fake.data[0], errG.data[0]))\n",
    "            print(\"Classify loss: {:5.2f} | Classify accuracy: {:3.3f}\\n\".format(\n",
    "                classify_loss, classify_acc))\n",
    "            with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "                f.write('[%d/%d][%d/%d] Loss_D: %.4f (Loss_D_real: %.4f '\n",
    "                        'Loss_D_fake: %.4f) Loss_G: %.4f\\n'\n",
    "                        % (epoch, args.epochs, niter, len(train1_data),\n",
    "                           errD.data[0], errD_real.data[0],\n",
    "                           errD_fake.data[0], errG.data[0]))\n",
    "                f.write(\"Classify loss: {:5.2f} | Classify accuracy: {:3.3f}\\n\".format(\n",
    "                        classify_loss, classify_acc))\n",
    "\n",
    "            # exponentially decaying noise on autoencoder\n",
    "            autoencoder.noise_r = \\\n",
    "                autoencoder.noise_r * args.noise_anneal\n",
    "\n",
    "    # end of epoch ----------------------------\n",
    "    # evaluation\n",
    "    test_loss, accuracy = evaluate_autoencoder(1, test1_data[:1000], epoch)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "          'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "          format(epoch, (time.time() - epoch_start_time),\n",
    "                 test_loss, math.exp(test_loss), accuracy))\n",
    "    print('-' * 89)\n",
    "    with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "                ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "                format(epoch, (time.time() - epoch_start_time),\n",
    "                       test_loss, math.exp(test_loss), accuracy))\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n')\n",
    "\n",
    "    test_loss, accuracy = evaluate_autoencoder(2, test2_data[:1000], epoch)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "          'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "          format(epoch, (time.time() - epoch_start_time),\n",
    "                 test_loss, math.exp(test_loss), accuracy))\n",
    "    print('-' * 89)\n",
    "    with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "                ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "                format(epoch, (time.time() - epoch_start_time),\n",
    "                       test_loss, math.exp(test_loss), accuracy))\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n')\n",
    "\n",
    "    evaluate_generator(1, fixed_noise, \"end_of_epoch_{}\".format(epoch))\n",
    "    evaluate_generator(2, fixed_noise, \"end_of_epoch_{}\".format(epoch))\n",
    "\n",
    "    # shuffle between epochs\n",
    "    train1_data = batchify(\n",
    "        corpus.data['train1'], args.batch_size, shuffle=True)\n",
    "    train2_data = batchify(\n",
    "        corpus.data['train2'], args.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_loss, accuracy = evaluate_autoencoder(1, test1_data, epoch + 1)\n",
    "print('-' * 89)\n",
    "print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "      'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "      format(epoch, (time.time() - epoch_start_time),\n",
    "             test_loss, math.exp(test_loss), accuracy))\n",
    "print('-' * 89)\n",
    "with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "            ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "            format(epoch, (time.time() - epoch_start_time),\n",
    "                   test_loss, math.exp(test_loss), accuracy))\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n')\n",
    "\n",
    "test_loss, accuracy = evaluate_autoencoder(2, test2_data, epoch + 1)\n",
    "print('-' * 89)\n",
    "print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "      'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "      format(epoch, (time.time() - epoch_start_time),\n",
    "             test_loss, math.exp(test_loss), accuracy))\n",
    "print('-' * 89)\n",
    "with open(\"{}/log.txt\".format(args.outf), 'a') as f:\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "            ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "            format(epoch, (time.time() - epoch_start_time),\n",
    "                   test_loss, math.exp(test_loss), accuracy))\n",
    "    f.write('-' * 89)\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
